-----------------------------------------------------------------------
Running window view
./train_all_grid.sh: line 76: syntax error: unexpected end of file
-----------------------------------------------------------------------
Running concatenated view
./train_all_grid.sh: line 76: syntax error: unexpected end of file
-----------------------------------------------------------------------
Running window view
Entering to /workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/har_classification
 ************ Training mlp1x64_fft on KuHar ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
Experiment with name har_train not found. Creating it.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name           | Type             | Params
----------------------------------------------------
0 | backbone       | Sequential       | 23.2 K
1 | backbone.fc1   | Linear           | 23.2 K
2 | backbone.relu1 | ReLU             | 0     
3 | fc             | Sequential       | 774   
4 | fc.0           | Linear           | 774   
5 | fc.1           | Softmax          | 0     
6 | loss_fn        | CrossEntropyLoss | 0     
----------------------------------------------------
23.9 K    Trainable params
0         Non-trainable params
23.9 K    Total params
0.096     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 164.108 seconds
Epoch 83/299 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11/11 0:00:00 • 0:00:00 326.51it/s v_num: f5cc val_loss: 1.444 train_loss: 1.220
 ************ Training mlp1x64_fft on MotionSense ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name           | Type             | Params
----------------------------------------------------
0 | backbone       | Sequential       | 23.2 K
1 | backbone.fc1   | Linear           | 23.2 K
2 | backbone.relu1 | ReLU             | 0     
3 | fc             | Sequential       | 774   
4 | fc.0           | Linear           | 774   
5 | fc.1           | Softmax          | 0     
6 | loss_fn        | CrossEntropyLoss | 0     
----------------------------------------------------
23.9 K    Trainable params
0         Non-trainable params
23.9 K    Total params
0.096     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (28) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 130.845 seconds
Epoch 63/299 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 28/28 0:00:01 • 0:00:00 386.26it/s v_num: 090d val_loss: 1.472 train_loss: 1.390
 ************ Training mlp1x64_fft on RealWorld_thigh ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name           | Type             | Params
----------------------------------------------------
0 | backbone       | Sequential       | 23.2 K
1 | backbone.fc1   | Linear           | 23.2 K
2 | backbone.relu1 | ReLU             | 0     
3 | fc             | Sequential       | 774   
4 | fc.0           | Linear           | 774   
5 | fc.1           | Softmax          | 0     
6 | loss_fn        | CrossEntropyLoss | 0     
----------------------------------------------------
23.9 K    Trainable params
0         Non-trainable params
23.9 K    Total params
0.096     Total estimated model params size (MB)
--> Overall fit time: 110.816 seconds
Epoch 49/299 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66/66 0:00:01 • 0:00:00 388.73it/s v_num: 54d7 val_loss: 1.445 train_loss: 1.236
 ************ Training mlp1x64_fft on RealWorld_waist ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name           | Type             | Params
----------------------------------------------------
0 | backbone       | Sequential       | 23.2 K
1 | backbone.fc1   | Linear           | 23.2 K
2 | backbone.relu1 | ReLU             | 0     
3 | fc             | Sequential       | 774   
4 | fc.0           | Linear           | 774   
5 | fc.1           | Softmax          | 0     
6 | loss_fn        | CrossEntropyLoss | 0     
----------------------------------------------------
23.9 K    Trainable params
0         Non-trainable params
23.9 K    Total params
0.096     Total estimated model params size (MB)
--> Overall fit time: 75.552 seconds
Epoch 32/299 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81/81 0:00:01 • 0:00:00 389.16it/s v_num: cc3c val_loss: 1.569 train_loss: 1.384
 ************ Training mlp1x64_fft on UCI ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name           | Type             | Params
----------------------------------------------------
0 | backbone       | Sequential       | 23.2 K
1 | backbone.fc1   | Linear           | 23.2 K
2 | backbone.relu1 | ReLU             | 0     
3 | fc             | Sequential       | 774   
4 | fc.0           | Linear           | 774   
5 | fc.1           | Softmax          | 0     
6 | loss_fn        | CrossEntropyLoss | 0     
----------------------------------------------------
23.9 K    Trainable params
0         Non-trainable params
23.9 K    Total params
0.096     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (19) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 269.865 seconds
Epoch 130/299 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19/19 0:00:00 • 0:00:00 387.08it/s v_num: 983e val_loss: 1.550 train_loss: 1.465
 ************ Training mlp1x64_fft on WISDM ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name           | Type             | Params
----------------------------------------------------
0 | backbone       | Sequential       | 23.2 K
1 | backbone.fc1   | Linear           | 23.2 K
2 | backbone.relu1 | ReLU             | 0     
3 | fc             | Sequential       | 774   
4 | fc.0           | Linear           | 774   
5 | fc.1           | Softmax          | 0     
6 | loss_fn        | CrossEntropyLoss | 0     
----------------------------------------------------
23.9 K    Trainable params
0         Non-trainable params
23.9 K    Total params
0.096     Total estimated model params size (MB)
Epoch 0/299                                          0/86 0:00:00 • -:--:-- 0.00it/s v_num: 40f5 val_loss: 1.781
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 579, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 986, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1032, in _run_stage
    self.fit_loop.run()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 138, in run
    self.advance(data_fetcher)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 242, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 191, in run
    self._optimizer_step(batch_idx, closure)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 269, in _optimizer_step
    call._call_lightning_module_hook(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 157, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/module.py", line 1303, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/optimizer.py", line 152, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 33, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py", line 123, in step
    loss = closure()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 108, in _wrap_closure
    closure_result = closure()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 144, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 129, in closure
    step_output = self._step_fn()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 391, in training_step
    return self.lightning_module.training_step(*args, **kwargs)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 69, in training_step
    return self.single_step(batch, batch_idx, "train")
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 57, in single_step
    self.log(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/module.py", line 516, in log
    results.log(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 406, in log
    self.register_key(key, meta, value)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 423, in register_key
    metric = _ResultMetric(meta, isinstance(value, Tensor)).to(value.device)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 311, in to
    self.__dict__.update(apply_to_collection(d, (Tensor, Metric), move_data_to_device, *args, **kwargs))
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 72, in apply_to_collection
    return _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 104, in _apply_to_collection_slow
    v = _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 104, in _apply_to_collection_slow
    v = _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 96, in _apply_to_collection_slow
    return function(data, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/apply_func.py", line 102, in move_data_to_device
    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 64, in apply_to_collection
    return function(data, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/apply_func.py", line 96, in batch_to
    data_output = data.to(device, **kwargs)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/har_classification/mlp.py", line 132, in <module>
    auto_main(options)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/cli.py", line 52, in auto_main
    return pipeline.run()
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/mlflow_train.py", line 180, in run
    trainer.fit(model, datamodule)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 543, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 68, in _call_and_handle_interrupt
    trainer._teardown()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1009, in _teardown
    self.strategy.teardown()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 537, in teardown
    self.lightning_module.cpu()
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/device_dtype_mixin.py", line 82, in cpu
    return super().cpu()
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 954, in cpu
    return self._apply(lambda t: t.cpu())
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 820, in _apply
    param_applied = fn(param)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 954, in <lambda>
    return self._apply(lambda t: t.cpu())
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [19,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [26,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [29,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [30,0,0] Assertion `t >= 0 && t < n_classes` failed.
Exception in thread Thread-2 (_pin_memory_loop):
Traceback (most recent call last):
  File "/usr/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py", line 54, in _pin_memory_loop
    do_one_step()
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py", line 31, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/usr/lib/python3.10/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/reductions.py", line 316, in rebuild_storage_fd
    fd = df.detach()
  File "/usr/lib/python3.10/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/usr/lib/python3.10/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 509, in Client
    deliver_challenge(c, authkey)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 740, in deliver_challenge
    response = connection.recv_bytes(256)        # reject large message
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer
 ************ Training tfchead_fft on KuHar ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name              | Type              | Params
--------------------------------------------------------
0 | backbone          | TFCPredictionHead | 12.0 K
1 | backbone.layers   | Sequential        | 12.0 K
2 | backbone.layers.0 | Linear            | 11.6 K
3 | backbone.layers.1 | Sigmoid           | 0     
4 | backbone.layers.2 | Linear            | 390   
5 | fc                | Identity          | 0     
6 | loss_fn           | CrossEntropyLoss  | 0     
--------------------------------------------------------
12.0 K    Trainable params
0         Non-trainable params
12.0 K    Total params
0.048     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 68.921 seconds
Epoch 35/299 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11/11 0:00:01 • 0:00:00 352.65it/s v_num: 0ae9 val_loss: 1.751 train_loss: 0.265
 ************ Training tfchead_fft on MotionSense ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name              | Type              | Params
--------------------------------------------------------
0 | backbone          | TFCPredictionHead | 12.0 K
1 | backbone.layers   | Sequential        | 12.0 K
2 | backbone.layers.0 | Linear            | 11.6 K
3 | backbone.layers.1 | Sigmoid           | 0     
4 | backbone.layers.2 | Linear            | 390   
5 | fc                | Identity          | 0     
6 | loss_fn           | CrossEntropyLoss  | 0     
--------------------------------------------------------
12.0 K    Trainable params
0         Non-trainable params
12.0 K    Total params
0.048     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (28) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 95.568 seconds
Epoch 46/299 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 28/28 0:00:00 • 0:00:00 423.29it/s v_num: 8a77 val_loss: 0.988 train_loss: 0.084
 ************ Training tfchead_fft on RealWorld_thigh ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name              | Type              | Params
--------------------------------------------------------
0 | backbone          | TFCPredictionHead | 12.0 K
1 | backbone.layers   | Sequential        | 12.0 K
2 | backbone.layers.0 | Linear            | 11.6 K
3 | backbone.layers.1 | Sigmoid           | 0     
4 | backbone.layers.2 | Linear            | 390   
5 | fc                | Identity          | 0     
6 | loss_fn           | CrossEntropyLoss  | 0     
--------------------------------------------------------
12.0 K    Trainable params
0         Non-trainable params
12.0 K    Total params
0.048     Total estimated model params size (MB)
--> Overall fit time: 72.799 seconds
Epoch 32/299 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66/66 0:00:01 • 0:00:00 477.09it/s v_num: d58a val_loss: 1.123 train_loss: 0.172
 ************ Training tfchead_fft on RealWorld_waist ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name              | Type              | Params
--------------------------------------------------------
0 | backbone          | TFCPredictionHead | 12.0 K
1 | backbone.layers   | Sequential        | 12.0 K
2 | backbone.layers.0 | Linear            | 11.6 K
3 | backbone.layers.1 | Sigmoid           | 0     
4 | backbone.layers.2 | Linear            | 390   
5 | fc                | Identity          | 0     
6 | loss_fn           | CrossEntropyLoss  | 0     
--------------------------------------------------------
12.0 K    Trainable params
0         Non-trainable params
12.0 K    Total params
0.048     Total estimated model params size (MB)
--> Overall fit time: 88.202 seconds
Epoch 38/299 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81/81 0:00:01 • 0:00:00 380.75it/s v_num: d799 val_loss: 0.804 train_loss: 0.153
 ************ Training tfchead_fft on UCI ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name              | Type              | Params
--------------------------------------------------------
0 | backbone          | TFCPredictionHead | 12.0 K
1 | backbone.layers   | Sequential        | 12.0 K
2 | backbone.layers.0 | Linear            | 11.6 K
3 | backbone.layers.1 | Sigmoid           | 0     
4 | backbone.layers.2 | Linear            | 390   
5 | fc                | Identity          | 0     
6 | loss_fn           | CrossEntropyLoss  | 0     
--------------------------------------------------------
12.0 K    Trainable params
0         Non-trainable params
12.0 K    Total params
0.048     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (19) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 120.485 seconds
Epoch 57/299 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19/19 0:00:01 • 0:00:00 307.02it/s v_num: 4e80 val_loss: 0.491 train_loss: 0.135
 ************ Training tfchead_fft on WISDM ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name              | Type              | Params
--------------------------------------------------------
0 | backbone          | TFCPredictionHead | 12.0 K
1 | backbone.layers   | Sequential        | 12.0 K
2 | backbone.layers.0 | Linear            | 11.6 K
3 | backbone.layers.1 | Sigmoid           | 0     
4 | backbone.layers.2 | Linear            | 390   
5 | fc                | Identity          | 0     
6 | loss_fn           | CrossEntropyLoss  | 0     
--------------------------------------------------------
12.0 K    Trainable params
0         Non-trainable params
12.0 K    Total params
0.048     Total estimated model params size (MB)
Epoch 0/299                                          0/86 0:00:00 • -:--:-- 0.00it/s v_num: 04d7 val_loss: 1.771
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 579, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 986, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1032, in _run_stage
    self.fit_loop.run()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 138, in run
    self.advance(data_fetcher)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 242, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 191, in run
    self._optimizer_step(batch_idx, closure)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 269, in _optimizer_step
    call._call_lightning_module_hook(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 157, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/module.py", line 1303, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/optimizer.py", line 152, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 33, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py", line 123, in step
    loss = closure()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 108, in _wrap_closure
    closure_result = closure()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 144, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 129, in closure
    step_output = self._step_fn()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 391, in training_step
    return self.lightning_module.training_step(*args, **kwargs)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 69, in training_step
    return self.single_step(batch, batch_idx, "train")
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 57, in single_step
    self.log(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/module.py", line 516, in log
    results.log(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 406, in log
    self.register_key(key, meta, value)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 423, in register_key
    metric = _ResultMetric(meta, isinstance(value, Tensor)).to(value.device)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 311, in to
    self.__dict__.update(apply_to_collection(d, (Tensor, Metric), move_data_to_device, *args, **kwargs))
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 72, in apply_to_collection
    return _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 104, in _apply_to_collection_slow
    v = _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 104, in _apply_to_collection_slow
    v = _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 96, in _apply_to_collection_slow
    return function(data, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/apply_func.py", line 102, in move_data_to_device
    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 64, in apply_to_collection
    return function(data, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/apply_func.py", line 96, in batch_to
    data_output = data.to(device, **kwargs)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/har_classification/tfc_head_classifier.py", line 132, in <module>
    auto_main(options)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/cli.py", line 52, in auto_main
    return pipeline.run()
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/mlflow_train.py", line 180, in run
    trainer.fit(model, datamodule)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 543, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 68, in _call_and_handle_interrupt
    trainer._teardown()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1009, in _teardown
    self.strategy.teardown()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 537, in teardown
    self.lightning_module.cpu()
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/device_dtype_mixin.py", line 82, in cpu
    return super().cpu()
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 954, in cpu
    return self._apply(lambda t: t.cpu())
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 820, in _apply
    param_applied = fn(param)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 954, in <lambda>
    return self._apply(lambda t: t.cpu())
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [12,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [13,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [14,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [18,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [23,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [27,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [30,0,0] Assertion `t >= 0 && t < n_classes` failed.
Exception in thread Thread-3 (_pin_memory_loop):
Traceback (most recent call last):
  File "/usr/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py", line 54, in _pin_memory_loop
    do_one_step()
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py", line 31, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/usr/lib/python3.10/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/reductions.py", line 316, in rebuild_storage_fd
    fd = df.detach()
  File "/usr/lib/python3.10/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/usr/lib/python3.10/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 509, in Client
    deliver_challenge(c, authkey)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 740, in deliver_challenge
    response = connection.recv_bytes(256)        # reject large message
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer
 ************ Training simple1Dconv_fft on KuHar ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name       | Type             | Params
-------------------------------------------------
0  | backbone   | Sequential       | 43.1 K
1  | backbone.0 | Conv1d           | 2.0 K 
2  | backbone.1 | ReLU             | 0     
3  | backbone.2 | Dropout          | 0     
4  | backbone.3 | Conv1d           | 20.5 K
5  | backbone.4 | ReLU             | 0     
6  | backbone.5 | Dropout          | 0     
7  | backbone.6 | Conv1d           | 20.5 K
8  | backbone.7 | ReLU             | 0     
9  | fc         | Sequential       | 148 K 
10 | fc.0       | Dropout          | 0     
11 | fc.1       | Linear           | 147 K 
12 | fc.2       | ReLU             | 0     
13 | fc.3       | Dropout          | 0     
14 | fc.4       | Linear           | 774   
15 | loss_fn    | CrossEntropyLoss | 0     
-------------------------------------------------
191 K     Trainable params
0         Non-trainable params
191 K     Total params
0.766     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 182.684 seconds
Epoch 78/299 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11/11 0:00:01 • 0:00:00 309.00it/s v_num: c215 val_loss: 0.881 train_loss: 0.101
 ************ Training simple1Dconv_fft on MotionSense ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name       | Type             | Params
-------------------------------------------------
0  | backbone   | Sequential       | 43.1 K
1  | backbone.0 | Conv1d           | 2.0 K 
2  | backbone.1 | ReLU             | 0     
3  | backbone.2 | Dropout          | 0     
4  | backbone.3 | Conv1d           | 20.5 K
5  | backbone.4 | ReLU             | 0     
6  | backbone.5 | Dropout          | 0     
7  | backbone.6 | Conv1d           | 20.5 K
8  | backbone.7 | ReLU             | 0     
9  | fc         | Sequential       | 148 K 
10 | fc.0       | Dropout          | 0     
11 | fc.1       | Linear           | 147 K 
12 | fc.2       | ReLU             | 0     
13 | fc.3       | Dropout          | 0     
14 | fc.4       | Linear           | 774   
15 | loss_fn    | CrossEntropyLoss | 0     
-------------------------------------------------
191 K     Trainable params
0         Non-trainable params
191 K     Total params
0.766     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (28) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 203.985 seconds
Epoch 85/299 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 28/28 0:00:01 • 0:00:00 220.24it/s v_num: 272f val_loss: 0.489 train_loss: 0.120
 ************ Training simple1Dconv_fft on RealWorld_thigh ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name       | Type             | Params
-------------------------------------------------
0  | backbone   | Sequential       | 43.1 K
1  | backbone.0 | Conv1d           | 2.0 K 
2  | backbone.1 | ReLU             | 0     
3  | backbone.2 | Dropout          | 0     
4  | backbone.3 | Conv1d           | 20.5 K
5  | backbone.4 | ReLU             | 0     
6  | backbone.5 | Dropout          | 0     
7  | backbone.6 | Conv1d           | 20.5 K
8  | backbone.7 | ReLU             | 0     
9  | fc         | Sequential       | 148 K 
10 | fc.0       | Dropout          | 0     
11 | fc.1       | Linear           | 147 K 
12 | fc.2       | ReLU             | 0     
13 | fc.3       | Dropout          | 0     
14 | fc.4       | Linear           | 774   
15 | loss_fn    | CrossEntropyLoss | 0     
-------------------------------------------------
191 K     Trainable params
0         Non-trainable params
191 K     Total params
0.766     Total estimated model params size (MB)
--> Overall fit time: 131.659 seconds
Epoch 49/299 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66/66 0:00:01 • 0:00:00 274.04it/s v_num: a9bc val_loss: 0.699 train_loss: 0.258
 ************ Training simple1Dconv_fft on RealWorld_waist ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name       | Type             | Params
-------------------------------------------------
0  | backbone   | Sequential       | 43.1 K
1  | backbone.0 | Conv1d           | 2.0 K 
2  | backbone.1 | ReLU             | 0     
3  | backbone.2 | Dropout          | 0     
4  | backbone.3 | Conv1d           | 20.5 K
5  | backbone.4 | ReLU             | 0     
6  | backbone.5 | Dropout          | 0     
7  | backbone.6 | Conv1d           | 20.5 K
8  | backbone.7 | ReLU             | 0     
9  | fc         | Sequential       | 148 K 
10 | fc.0       | Dropout          | 0     
11 | fc.1       | Linear           | 147 K 
12 | fc.2       | ReLU             | 0     
13 | fc.3       | Dropout          | 0     
14 | fc.4       | Linear           | 774   
15 | loss_fn    | CrossEntropyLoss | 0     
-------------------------------------------------
191 K     Trainable params
0         Non-trainable params
191 K     Total params
0.766     Total estimated model params size (MB)
--> Overall fit time: 144.418 seconds
Epoch 53/299 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81/81 0:00:01 • 0:00:00 232.15it/s v_num: b0ca val_loss: 0.444 train_loss: 0.256
 ************ Training simple1Dconv_fft on UCI ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name       | Type             | Params
-------------------------------------------------
0  | backbone   | Sequential       | 43.1 K
1  | backbone.0 | Conv1d           | 2.0 K 
2  | backbone.1 | ReLU             | 0     
3  | backbone.2 | Dropout          | 0     
4  | backbone.3 | Conv1d           | 20.5 K
5  | backbone.4 | ReLU             | 0     
6  | backbone.5 | Dropout          | 0     
7  | backbone.6 | Conv1d           | 20.5 K
8  | backbone.7 | ReLU             | 0     
9  | fc         | Sequential       | 148 K 
10 | fc.0       | Dropout          | 0     
11 | fc.1       | Linear           | 147 K 
12 | fc.2       | ReLU             | 0     
13 | fc.3       | Dropout          | 0     
14 | fc.4       | Linear           | 774   
15 | loss_fn    | CrossEntropyLoss | 0     
-------------------------------------------------
191 K     Trainable params
0         Non-trainable params
191 K     Total params
0.766     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (19) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 162.157 seconds
Epoch 62/299 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19/19 0:00:01 • 0:00:00 234.02it/s v_num: 82e5 val_loss: 0.309 train_loss: 0.144
 ************ Training simple1Dconv_fft on WISDM ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name       | Type             | Params
-------------------------------------------------
0  | backbone   | Sequential       | 43.1 K
1  | backbone.0 | Conv1d           | 2.0 K 
2  | backbone.1 | ReLU             | 0     
3  | backbone.2 | Dropout          | 0     
4  | backbone.3 | Conv1d           | 20.5 K
5  | backbone.4 | ReLU             | 0     
6  | backbone.5 | Dropout          | 0     
7  | backbone.6 | Conv1d           | 20.5 K
8  | backbone.7 | ReLU             | 0     
9  | fc         | Sequential       | 148 K 
10 | fc.0       | Dropout          | 0     
11 | fc.1       | Linear           | 147 K 
12 | fc.2       | ReLU             | 0     
13 | fc.3       | Dropout          | 0     
14 | fc.4       | Linear           | 774   
15 | loss_fn    | CrossEntropyLoss | 0     
-------------------------------------------------
191 K     Trainable params
0         Non-trainable params
191 K     Total params
0.766     Total estimated model params size (MB)
Epoch 0/299                                          0/86 0:00:00 • -:--:-- 0.00it/s v_num: ab8f val_loss: 1.798
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 579, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 986, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1032, in _run_stage
    self.fit_loop.run()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 138, in run
    self.advance(data_fetcher)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 242, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 191, in run
    self._optimizer_step(batch_idx, closure)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 269, in _optimizer_step
    call._call_lightning_module_hook(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 157, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/module.py", line 1303, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/optimizer.py", line 152, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 33, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py", line 123, in step
    loss = closure()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 108, in _wrap_closure
    closure_result = closure()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 144, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 129, in closure
    step_output = self._step_fn()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 391, in training_step
    return self.lightning_module.training_step(*args, **kwargs)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 69, in training_step
    return self.single_step(batch, batch_idx, "train")
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 57, in single_step
    self.log(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/module.py", line 516, in log
    results.log(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 406, in log
    self.register_key(key, meta, value)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 423, in register_key
    metric = _ResultMetric(meta, isinstance(value, Tensor)).to(value.device)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 311, in to
    self.__dict__.update(apply_to_collection(d, (Tensor, Metric), move_data_to_device, *args, **kwargs))
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 72, in apply_to_collection
    return _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 104, in _apply_to_collection_slow
    v = _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 104, in _apply_to_collection_slow
    v = _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 96, in _apply_to_collection_slow
    return function(data, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/apply_func.py", line 102, in move_data_to_device
    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 64, in apply_to_collection
    return function(data, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/apply_func.py", line 96, in batch_to
    data_output = data.to(device, **kwargs)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/har_classification/simple1Dconv_classifier.py", line 128, in <module>
    auto_main(options)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/cli.py", line 52, in auto_main
    return pipeline.run()
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/mlflow_train.py", line 180, in run
    trainer.fit(model, datamodule)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 543, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 68, in _call_and_handle_interrupt
    trainer._teardown()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1009, in _teardown
    self.strategy.teardown()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 537, in teardown
    self.lightning_module.cpu()
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/device_dtype_mixin.py", line 82, in cpu
    return super().cpu()
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 954, in cpu
    return self._apply(lambda t: t.cpu())
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 820, in _apply
    param_applied = fn(param)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 954, in <lambda>
    return self._apply(lambda t: t.cpu())
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [12,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [18,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [21,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [27,0,0] Assertion `t >= 0 && t < n_classes` failed.
Exception in thread Thread-2 (_pin_memory_loop):
Traceback (most recent call last):
  File "/usr/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py", line 54, in _pin_memory_loop
    do_one_step()
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py", line 31, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/usr/lib/python3.10/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/reductions.py", line 316, in rebuild_storage_fd
    fd = df.detach()
  File "/usr/lib/python3.10/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/usr/lib/python3.10/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 508, in Client
    answer_challenge(c, authkey)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 757, in answer_challenge
    response = connection.recv_bytes(256)        # reject large message
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer
 ************ Training tfchead on KuHar ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name              | Type              | Params
--------------------------------------------------------
0 | backbone          | TFCPredictionHead | 23.5 K
1 | backbone.layers   | Sequential        | 23.5 K
2 | backbone.layers.0 | Linear            | 23.1 K
3 | backbone.layers.1 | Sigmoid           | 0     
4 | backbone.layers.2 | Linear            | 390   
5 | fc                | Identity          | 0     
6 | loss_fn           | CrossEntropyLoss  | 0     
--------------------------------------------------------
23.5 K    Trainable params
0         Non-trainable params
23.5 K    Total params
0.094     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 69.564 seconds
Epoch 30/299 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11/11 0:00:01 • 0:00:00 450.75it/s v_num: b195 val_loss: 2.997 train_loss: 0.567
 ************ Training tfchead on MotionSense ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name              | Type              | Params
--------------------------------------------------------
0 | backbone          | TFCPredictionHead | 23.5 K
1 | backbone.layers   | Sequential        | 23.5 K
2 | backbone.layers.0 | Linear            | 23.1 K
3 | backbone.layers.1 | Sigmoid           | 0     
4 | backbone.layers.2 | Linear            | 390   
5 | fc                | Identity          | 0     
6 | loss_fn           | CrossEntropyLoss  | 0     
--------------------------------------------------------
23.5 K    Trainable params
0         Non-trainable params
23.5 K    Total params
0.094     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (28) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 142.444 seconds
Epoch 60/299 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 28/28 0:00:01 • 0:00:00 459.16it/s v_num: 5d8e val_loss: 1.478 train_loss: 0.288
 ************ Training tfchead on RealWorld_thigh ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name              | Type              | Params
--------------------------------------------------------
0 | backbone          | TFCPredictionHead | 23.5 K
1 | backbone.layers   | Sequential        | 23.5 K
2 | backbone.layers.0 | Linear            | 23.1 K
3 | backbone.layers.1 | Sigmoid           | 0     
4 | backbone.layers.2 | Linear            | 390   
5 | fc                | Identity          | 0     
6 | loss_fn           | CrossEntropyLoss  | 0     
--------------------------------------------------------
23.5 K    Trainable params
0         Non-trainable params
23.5 K    Total params
0.094     Total estimated model params size (MB)
--> Overall fit time: 125.701 seconds
Epoch 50/299 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66/66 0:00:01 • 0:00:00 389.53it/s v_num: 1734 val_loss: 1.459 train_loss: 0.309
 ************ Training tfchead on RealWorld_waist ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name              | Type              | Params
--------------------------------------------------------
0 | backbone          | TFCPredictionHead | 23.5 K
1 | backbone.layers   | Sequential        | 23.5 K
2 | backbone.layers.0 | Linear            | 23.1 K
3 | backbone.layers.1 | Sigmoid           | 0     
4 | backbone.layers.2 | Linear            | 390   
5 | fc                | Identity          | 0     
6 | loss_fn           | CrossEntropyLoss  | 0     
--------------------------------------------------------
23.5 K    Trainable params
0         Non-trainable params
23.5 K    Total params
0.094     Total estimated model params size (MB)
--> Overall fit time: 139.439 seconds
Epoch 55/299 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81/81 0:00:01 • 0:00:00 408.75it/s v_num: da20 val_loss: 1.177 train_loss: 0.318
 ************ Training tfchead on UCI ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name              | Type              | Params
--------------------------------------------------------
0 | backbone          | TFCPredictionHead | 23.5 K
1 | backbone.layers   | Sequential        | 23.5 K
2 | backbone.layers.0 | Linear            | 23.1 K
3 | backbone.layers.1 | Sigmoid           | 0     
4 | backbone.layers.2 | Linear            | 390   
5 | fc                | Identity          | 0     
6 | loss_fn           | CrossEntropyLoss  | 0     
--------------------------------------------------------
23.5 K    Trainable params
0         Non-trainable params
23.5 K    Total params
0.094     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (19) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 308.848 seconds
Epoch 136/299 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19/19 0:00:01 • 0:00:00 359.88it/s v_num: e1b3 val_loss: 0.840 train_loss: 0.259
 ************ Training tfchead on WISDM ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name              | Type              | Params
--------------------------------------------------------
0 | backbone          | TFCPredictionHead | 23.5 K
1 | backbone.layers   | Sequential        | 23.5 K
2 | backbone.layers.0 | Linear            | 23.1 K
3 | backbone.layers.1 | Sigmoid           | 0     
4 | backbone.layers.2 | Linear            | 390   
5 | fc                | Identity          | 0     
6 | loss_fn           | CrossEntropyLoss  | 0     
--------------------------------------------------------
23.5 K    Trainable params
0         Non-trainable params
23.5 K    Total params
0.094     Total estimated model params size (MB)
Epoch 0/299                                          0/86 0:00:00 • -:--:-- 0.00it/s v_num: 9467 val_loss: 1.781
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 579, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 986, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1032, in _run_stage
    self.fit_loop.run()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 138, in run
    self.advance(data_fetcher)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 242, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 191, in run
    self._optimizer_step(batch_idx, closure)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 269, in _optimizer_step
    call._call_lightning_module_hook(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 157, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/module.py", line 1303, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/optimizer.py", line 152, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 33, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py", line 123, in step
    loss = closure()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 108, in _wrap_closure
    closure_result = closure()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 144, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 129, in closure
    step_output = self._step_fn()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 391, in training_step
    return self.lightning_module.training_step(*args, **kwargs)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 69, in training_step
    return self.single_step(batch, batch_idx, "train")
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 57, in single_step
    self.log(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/module.py", line 516, in log
    results.log(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 406, in log
    self.register_key(key, meta, value)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 423, in register_key
    metric = _ResultMetric(meta, isinstance(value, Tensor)).to(value.device)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 311, in to
    self.__dict__.update(apply_to_collection(d, (Tensor, Metric), move_data_to_device, *args, **kwargs))
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 72, in apply_to_collection
    return _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 104, in _apply_to_collection_slow
    v = _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 104, in _apply_to_collection_slow
    v = _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 96, in _apply_to_collection_slow
    return function(data, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/apply_func.py", line 102, in move_data_to_device
    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 64, in apply_to_collection
    return function(data, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/apply_func.py", line 96, in batch_to
    data_output = data.to(device, **kwargs)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/har_classification/tfc_head_classifier.py", line 132, in <module>
    auto_main(options)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/cli.py", line 52, in auto_main
    return pipeline.run()
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/mlflow_train.py", line 180, in run
    trainer.fit(model, datamodule)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 543, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 68, in _call_and_handle_interrupt
    trainer._teardown()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1009, in _teardown
    self.strategy.teardown()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 537, in teardown
    self.lightning_module.cpu()
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/device_dtype_mixin.py", line 82, in cpu
    return super().cpu()
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 954, in cpu
    return self._apply(lambda t: t.cpu())
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 820, in _apply
    param_applied = fn(param)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 954, in <lambda>
    return self._apply(lambda t: t.cpu())
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [11,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [12,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [13,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [16,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [17,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [19,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [21,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [22,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [25,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [26,0,0] Assertion `t >= 0 && t < n_classes` failed.
Exception in thread Thread-3 (_pin_memory_loop):
Traceback (most recent call last):
  File "/usr/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py", line 54, in _pin_memory_loop
    do_one_step()
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py", line 31, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/usr/lib/python3.10/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/reductions.py", line 316, in rebuild_storage_fd
    fd = df.detach()
  File "/usr/lib/python3.10/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/usr/lib/python3.10/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 509, in Client
    deliver_challenge(c, authkey)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 740, in deliver_challenge
    response = connection.recv_bytes(256)        # reject large message
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer
 ************ Training simple1Dconv on KuHar ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name       | Type             | Params
-------------------------------------------------
0  | backbone   | Sequential       | 43.1 K
1  | backbone.0 | Conv1d           | 2.0 K 
2  | backbone.1 | ReLU             | 0     
3  | backbone.2 | Dropout          | 0     
4  | backbone.3 | Conv1d           | 20.5 K
5  | backbone.4 | ReLU             | 0     
6  | backbone.5 | Dropout          | 0     
7  | backbone.6 | Conv1d           | 20.5 K
8  | backbone.7 | ReLU             | 0     
9  | fc         | Sequential       | 394 K 
10 | fc.0       | Dropout          | 0     
11 | fc.1       | Linear           | 393 K 
12 | fc.2       | ReLU             | 0     
13 | fc.3       | Dropout          | 0     
14 | fc.4       | Linear           | 774   
15 | loss_fn    | CrossEntropyLoss | 0     
-------------------------------------------------
437 K     Trainable params
0         Non-trainable params
437 K     Total params
1.749     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 79.234 seconds
Epoch 30/299 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11/11 0:00:01 • 0:00:00 280.64it/s v_num: 0b30 val_loss: 5.740 train_loss: 0.281
 ************ Training simple1Dconv on MotionSense ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name       | Type             | Params
-------------------------------------------------
0  | backbone   | Sequential       | 43.1 K
1  | backbone.0 | Conv1d           | 2.0 K 
2  | backbone.1 | ReLU             | 0     
3  | backbone.2 | Dropout          | 0     
4  | backbone.3 | Conv1d           | 20.5 K
5  | backbone.4 | ReLU             | 0     
6  | backbone.5 | Dropout          | 0     
7  | backbone.6 | Conv1d           | 20.5 K
8  | backbone.7 | ReLU             | 0     
9  | fc         | Sequential       | 394 K 
10 | fc.0       | Dropout          | 0     
11 | fc.1       | Linear           | 393 K 
12 | fc.2       | ReLU             | 0     
13 | fc.3       | Dropout          | 0     
14 | fc.4       | Linear           | 774   
15 | loss_fn    | CrossEntropyLoss | 0     
-------------------------------------------------
437 K     Trainable params
0         Non-trainable params
437 K     Total params
1.749     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (28) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 131.629 seconds
Epoch 48/299 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 28/28 0:00:01 • 0:00:00 302.84it/s v_num: a89e val_loss: 0.876 train_loss: 0.105
 ************ Training simple1Dconv on RealWorld_thigh ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name       | Type             | Params
-------------------------------------------------
0  | backbone   | Sequential       | 43.1 K
1  | backbone.0 | Conv1d           | 2.0 K 
2  | backbone.1 | ReLU             | 0     
3  | backbone.2 | Dropout          | 0     
4  | backbone.3 | Conv1d           | 20.5 K
5  | backbone.4 | ReLU             | 0     
6  | backbone.5 | Dropout          | 0     
7  | backbone.6 | Conv1d           | 20.5 K
8  | backbone.7 | ReLU             | 0     
9  | fc         | Sequential       | 394 K 
10 | fc.0       | Dropout          | 0     
11 | fc.1       | Linear           | 393 K 
12 | fc.2       | ReLU             | 0     
13 | fc.3       | Dropout          | 0     
14 | fc.4       | Linear           | 774   
15 | loss_fn    | CrossEntropyLoss | 0     
-------------------------------------------------
437 K     Trainable params
0         Non-trainable params
437 K     Total params
1.749     Total estimated model params size (MB)
--> Overall fit time: 89.616 seconds
Epoch 31/299 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66/66 0:00:01 • 0:00:00 199.65it/s v_num: 26e2 val_loss: 1.459 train_loss: 0.148
 ************ Training simple1Dconv on RealWorld_waist ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name       | Type             | Params
-------------------------------------------------
0  | backbone   | Sequential       | 43.1 K
1  | backbone.0 | Conv1d           | 2.0 K 
2  | backbone.1 | ReLU             | 0     
3  | backbone.2 | Dropout          | 0     
4  | backbone.3 | Conv1d           | 20.5 K
5  | backbone.4 | ReLU             | 0     
6  | backbone.5 | Dropout          | 0     
7  | backbone.6 | Conv1d           | 20.5 K
8  | backbone.7 | ReLU             | 0     
9  | fc         | Sequential       | 394 K 
10 | fc.0       | Dropout          | 0     
11 | fc.1       | Linear           | 393 K 
12 | fc.2       | ReLU             | 0     
13 | fc.3       | Dropout          | 0     
14 | fc.4       | Linear           | 774   
15 | loss_fn    | CrossEntropyLoss | 0     
-------------------------------------------------
437 K     Trainable params
0         Non-trainable params
437 K     Total params
1.749     Total estimated model params size (MB)
--> Overall fit time: 108.971 seconds
Epoch 37/299 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81/81 0:00:01 • 0:00:00 204.32it/s v_num: 3cba val_loss: 1.037 train_loss: 0.194
 ************ Training simple1Dconv on UCI ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name       | Type             | Params
-------------------------------------------------
0  | backbone   | Sequential       | 43.1 K
1  | backbone.0 | Conv1d           | 2.0 K 
2  | backbone.1 | ReLU             | 0     
3  | backbone.2 | Dropout          | 0     
4  | backbone.3 | Conv1d           | 20.5 K
5  | backbone.4 | ReLU             | 0     
6  | backbone.5 | Dropout          | 0     
7  | backbone.6 | Conv1d           | 20.5 K
8  | backbone.7 | ReLU             | 0     
9  | fc         | Sequential       | 394 K 
10 | fc.0       | Dropout          | 0     
11 | fc.1       | Linear           | 393 K 
12 | fc.2       | ReLU             | 0     
13 | fc.3       | Dropout          | 0     
14 | fc.4       | Linear           | 774   
15 | loss_fn    | CrossEntropyLoss | 0     
-------------------------------------------------
437 K     Trainable params
0         Non-trainable params
437 K     Total params
1.749     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (19) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 336.437 seconds
Epoch 130/299 ━━━━━━━━━━━━━━━━━━ 19/19 0:00:01 • 0:00:00 242.15it/s v_num: 0505        
                                                                    val_loss: 0.126    
                                                                    train_loss: 0.053  
 ************ Training simple1Dconv on WISDM ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name       | Type             | Params
-------------------------------------------------
0  | backbone   | Sequential       | 43.1 K
1  | backbone.0 | Conv1d           | 2.0 K 
2  | backbone.1 | ReLU             | 0     
3  | backbone.2 | Dropout          | 0     
4  | backbone.3 | Conv1d           | 20.5 K
5  | backbone.4 | ReLU             | 0     
6  | backbone.5 | Dropout          | 0     
7  | backbone.6 | Conv1d           | 20.5 K
8  | backbone.7 | ReLU             | 0     
9  | fc         | Sequential       | 394 K 
10 | fc.0       | Dropout          | 0     
11 | fc.1       | Linear           | 393 K 
12 | fc.2       | ReLU             | 0     
13 | fc.3       | Dropout          | 0     
14 | fc.4       | Linear           | 774   
15 | loss_fn    | CrossEntropyLoss | 0     
-------------------------------------------------
437 K     Trainable params
0         Non-trainable params
437 K     Total params
1.749     Total estimated model params size (MB)
Epoch 0/299                      0/86 0:00:00 • -:--:-- 0.00it/s v_num: fdbb val_loss: 
                                                                 1.748                 
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 579, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 986, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1032, in _run_stage
    self.fit_loop.run()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 138, in run
    self.advance(data_fetcher)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 242, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 191, in run
    self._optimizer_step(batch_idx, closure)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 269, in _optimizer_step
    call._call_lightning_module_hook(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 157, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/module.py", line 1303, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/optimizer.py", line 152, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 33, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py", line 123, in step
    loss = closure()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 108, in _wrap_closure
    closure_result = closure()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 144, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 129, in closure
    step_output = self._step_fn()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 391, in training_step
    return self.lightning_module.training_step(*args, **kwargs)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 69, in training_step
    return self.single_step(batch, batch_idx, "train")
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 57, in single_step
    self.log(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/module.py", line 516, in log
    results.log(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 406, in log
    self.register_key(key, meta, value)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 423, in register_key
    metric = _ResultMetric(meta, isinstance(value, Tensor)).to(value.device)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 311, in to
    self.__dict__.update(apply_to_collection(d, (Tensor, Metric), move_data_to_device, *args, **kwargs))
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 72, in apply_to_collection
    return _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 104, in _apply_to_collection_slow
    v = _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 104, in _apply_to_collection_slow
    v = _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 96, in _apply_to_collection_slow
    return function(data, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/apply_func.py", line 102, in move_data_to_device
    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 64, in apply_to_collection
    return function(data, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/apply_func.py", line 96, in batch_to
    data_output = data.to(device, **kwargs)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/har_classification/simple1Dconv_classifier.py", line 128, in <module>
    auto_main(options)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/cli.py", line 52, in auto_main
    return pipeline.run()
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/mlflow_train.py", line 180, in run
    trainer.fit(model, datamodule)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 543, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 68, in _call_and_handle_interrupt
    trainer._teardown()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1009, in _teardown
    self.strategy.teardown()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 537, in teardown
    self.lightning_module.cpu()
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/device_dtype_mixin.py", line 82, in cpu
    return super().cpu()
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 954, in cpu
    return self._apply(lambda t: t.cpu())
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 820, in _apply
    param_applied = fn(param)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 954, in <lambda>
    return self._apply(lambda t: t.cpu())
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [17,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [22,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [24,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [28,0,0] Assertion `t >= 0 && t < n_classes` failed.
Exception in thread Thread-2 (_pin_memory_loop):
Traceback (most recent call last):
  File "/usr/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py", line 54, in _pin_memory_loop
    do_one_step()
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py", line 31, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/usr/lib/python3.10/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/reductions.py", line 316, in rebuild_storage_fd
    fd = df.detach()
  File "/usr/lib/python3.10/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/usr/lib/python3.10/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 508, in Client
    answer_challenge(c, authkey)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 757, in answer_challenge
    response = connection.recv_bytes(256)        # reject large message
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer
 ************ Training tnchead on KuHar ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name              | Type              | Params
--------------------------------------------------------
0 | backbone          | TNCPredictionHead | 27.7 K
1 | backbone.layers   | Sequential        | 27.7 K
2 | backbone.layers.0 | Linear            | 23.1 K
3 | backbone.layers.1 | ReLU              | 0     
4 | backbone.layers.2 | Linear            | 4.2 K 
5 | backbone.layers.3 | Sequential        | 0     
6 | backbone.layers.4 | Linear            | 390   
7 | backbone.layers.5 | Softmax           | 0     
8 | fc                | Identity          | 0     
9 | loss_fn           | CrossEntropyLoss  | 0     
--------------------------------------------------------
27.7 K    Trainable params
0         Non-trainable params
27.7 K    Total params
0.111     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 116.237 seconds
Epoch 50/299 ━━━━━━━━━━━━━━━━━━ 11/11 0:00:01 • 0:00:00 373.45it/s v_num: b1e0         
                                                                   val_loss: 1.630     
                                                                   train_loss: 1.192   
 ************ Training tnchead on MotionSense ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name              | Type              | Params
--------------------------------------------------------
0 | backbone          | TNCPredictionHead | 27.7 K
1 | backbone.layers   | Sequential        | 27.7 K
2 | backbone.layers.0 | Linear            | 23.1 K
3 | backbone.layers.1 | ReLU              | 0     
4 | backbone.layers.2 | Linear            | 4.2 K 
5 | backbone.layers.3 | Sequential        | 0     
6 | backbone.layers.4 | Linear            | 390   
7 | backbone.layers.5 | Softmax           | 0     
8 | fc                | Identity          | 0     
9 | loss_fn           | CrossEntropyLoss  | 0     
--------------------------------------------------------
27.7 K    Trainable params
0         Non-trainable params
27.7 K    Total params
0.111     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (28) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 205.549 seconds
Epoch 85/299 ━━━━━━━━━━━━━━━━━━ 28/28 0:00:01 • 0:00:00 352.40it/s v_num: f944         
                                                                   val_loss: 1.349     
                                                                   train_loss: 1.075   
 ************ Training tnchead on RealWorld_thigh ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name              | Type              | Params
--------------------------------------------------------
0 | backbone          | TNCPredictionHead | 27.7 K
1 | backbone.layers   | Sequential        | 27.7 K
2 | backbone.layers.0 | Linear            | 23.1 K
3 | backbone.layers.1 | ReLU              | 0     
4 | backbone.layers.2 | Linear            | 4.2 K 
5 | backbone.layers.3 | Sequential        | 0     
6 | backbone.layers.4 | Linear            | 390   
7 | backbone.layers.5 | Softmax           | 0     
8 | fc                | Identity          | 0     
9 | loss_fn           | CrossEntropyLoss  | 0     
--------------------------------------------------------
27.7 K    Trainable params
0         Non-trainable params
27.7 K    Total params
0.111     Total estimated model params size (MB)
--> Overall fit time: 83.709 seconds
Epoch 32/299 ━━━━━━━━━━━━━━━━━━ 66/66 0:00:01 • 0:00:00 359.86it/s v_num: 3475         
                                                                   val_loss: 1.494     
                                                                   train_loss: 1.141   
 ************ Training tnchead on RealWorld_waist ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name              | Type              | Params
--------------------------------------------------------
0 | backbone          | TNCPredictionHead | 27.7 K
1 | backbone.layers   | Sequential        | 27.7 K
2 | backbone.layers.0 | Linear            | 23.1 K
3 | backbone.layers.1 | ReLU              | 0     
4 | backbone.layers.2 | Linear            | 4.2 K 
5 | backbone.layers.3 | Sequential        | 0     
6 | backbone.layers.4 | Linear            | 390   
7 | backbone.layers.5 | Softmax           | 0     
8 | fc                | Identity          | 0     
9 | loss_fn           | CrossEntropyLoss  | 0     
--------------------------------------------------------
27.7 K    Trainable params
0         Non-trainable params
27.7 K    Total params
0.111     Total estimated model params size (MB)
--> Overall fit time: 157.098 seconds
Epoch 59/299 ━━━━━━━━━━━━━━━━━━ 81/81 0:00:01 • 0:00:00 360.47it/s v_num: 196a         
                                                                   val_loss: 1.322     
                                                                   train_loss: 1.111   
 ************ Training tnchead on UCI ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name              | Type              | Params
--------------------------------------------------------
0 | backbone          | TNCPredictionHead | 27.7 K
1 | backbone.layers   | Sequential        | 27.7 K
2 | backbone.layers.0 | Linear            | 23.1 K
3 | backbone.layers.1 | ReLU              | 0     
4 | backbone.layers.2 | Linear            | 4.2 K 
5 | backbone.layers.3 | Sequential        | 0     
6 | backbone.layers.4 | Linear            | 390   
7 | backbone.layers.5 | Softmax           | 0     
8 | fc                | Identity          | 0     
9 | loss_fn           | CrossEntropyLoss  | 0     
--------------------------------------------------------
27.7 K    Trainable params
0         Non-trainable params
27.7 K    Total params
0.111     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (19) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 204.483 seconds
Epoch 88/299 ━━━━━━━━━━━━━━━━━━ 19/19 0:00:01 • 0:00:00 354.32it/s v_num: 0458         
                                                                   val_loss: 1.215     
                                                                   train_loss: 1.051   
 ************ Training tnchead on WISDM ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name              | Type              | Params
--------------------------------------------------------
0 | backbone          | TNCPredictionHead | 27.7 K
1 | backbone.layers   | Sequential        | 27.7 K
2 | backbone.layers.0 | Linear            | 23.1 K
3 | backbone.layers.1 | ReLU              | 0     
4 | backbone.layers.2 | Linear            | 4.2 K 
5 | backbone.layers.3 | Sequential        | 0     
6 | backbone.layers.4 | Linear            | 390   
7 | backbone.layers.5 | Softmax           | 0     
8 | fc                | Identity          | 0     
9 | loss_fn           | CrossEntropyLoss  | 0     
--------------------------------------------------------
27.7 K    Trainable params
0         Non-trainable params
27.7 K    Total params
0.111     Total estimated model params size (MB)
Epoch 0/299                      0/86 0:00:00 • -:--:-- 0.00it/s v_num: afb3 val_loss: 
                                                                 1.767                 
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 579, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 986, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1032, in _run_stage
    self.fit_loop.run()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 138, in run
    self.advance(data_fetcher)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 242, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 191, in run
    self._optimizer_step(batch_idx, closure)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 269, in _optimizer_step
    call._call_lightning_module_hook(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 157, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/module.py", line 1303, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/optimizer.py", line 152, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 33, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py", line 123, in step
    loss = closure()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 108, in _wrap_closure
    closure_result = closure()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 144, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 129, in closure
    step_output = self._step_fn()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 391, in training_step
    return self.lightning_module.training_step(*args, **kwargs)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 69, in training_step
    return self.single_step(batch, batch_idx, "train")
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 57, in single_step
    self.log(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/module.py", line 516, in log
    results.log(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 406, in log
    self.register_key(key, meta, value)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 423, in register_key
    metric = _ResultMetric(meta, isinstance(value, Tensor)).to(value.device)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 311, in to
    self.__dict__.update(apply_to_collection(d, (Tensor, Metric), move_data_to_device, *args, **kwargs))
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 72, in apply_to_collection
    return _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 104, in _apply_to_collection_slow
    v = _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 104, in _apply_to_collection_slow
    v = _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 96, in _apply_to_collection_slow
    return function(data, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/apply_func.py", line 102, in move_data_to_device
    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 64, in apply_to_collection
    return function(data, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/apply_func.py", line 96, in batch_to
    data_output = data.to(device, **kwargs)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/har_classification/tnc_head_classifier.py", line 131, in <module>
    auto_main(options)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/cli.py", line 52, in auto_main
    return pipeline.run()
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/mlflow_train.py", line 180, in run
    trainer.fit(model, datamodule)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 543, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 68, in _call_and_handle_interrupt
    trainer._teardown()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1009, in _teardown
    self.strategy.teardown()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 537, in teardown
    self.lightning_module.cpu()
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/device_dtype_mixin.py", line 82, in cpu
    return super().cpu()
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 954, in cpu
    return self._apply(lambda t: t.cpu())
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 820, in _apply
    param_applied = fn(param)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 954, in <lambda>
    return self._apply(lambda t: t.cpu())
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [11,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [13,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [24,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [31,0,0] Assertion `t >= 0 && t < n_classes` failed.
Exception in thread Thread-3 (_pin_memory_loop):
Traceback (most recent call last):
  File "/usr/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py", line 54, in _pin_memory_loop
    do_one_step()
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py", line 31, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/usr/lib/python3.10/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/reductions.py", line 316, in rebuild_storage_fd
    fd = df.detach()
  File "/usr/lib/python3.10/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/usr/lib/python3.10/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 508, in Client
    answer_challenge(c, authkey)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 752, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer
 ************ Training tnchead_fft on KuHar ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name              | Type              | Params
--------------------------------------------------------
0 | backbone          | TNCPredictionHead | 27.7 K
1 | backbone.layers   | Sequential        | 27.7 K
2 | backbone.layers.0 | Linear            | 23.1 K
3 | backbone.layers.1 | ReLU              | 0     
4 | backbone.layers.2 | Linear            | 4.2 K 
5 | backbone.layers.3 | Sequential        | 0     
6 | backbone.layers.4 | Linear            | 390   
7 | backbone.layers.5 | Softmax           | 0     
8 | fc                | Identity          | 0     
9 | loss_fn           | CrossEntropyLoss  | 0     
--------------------------------------------------------
27.7 K    Trainable params
0         Non-trainable params
27.7 K    Total params
0.111     Total estimated model params size (MB)

Traceback (most recent call last):
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/har_classification/tnc_head_classifier.py", line 131, in <module>
    auto_main(options)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/cli.py", line 52, in auto_main
    return pipeline.run()
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/mlflow_train.py", line 180, in run
    trainer.fit(model, datamodule)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 543, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 579, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 986, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1030, in _run_stage
    self._run_sanity_check()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1059, in _run_sanity_check
    val_loop.run()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/utilities.py", line 182, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/evaluation_loop.py", line 135, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/evaluation_loop.py", line 396, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 412, in validation_step
    return self.lightning_module.validation_step(*args, **kwargs)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 72, in validation_step
    return self.single_step(batch, batch_idx, "val")
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 55, in single_step
    y_hat = self.forward(x)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 35, in forward
    x = self.backbone(x)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/vscode/.local/lib/python3.10/site-packages/lightly/models/modules/heads.py", line 65, in forward
    projection: Tensor = self.layers(x)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (128x180 and 360x64)
 ************ Training tnchead_fft on MotionSense ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name              | Type              | Params
--------------------------------------------------------
0 | backbone          | TNCPredictionHead | 27.7 K
1 | backbone.layers   | Sequential        | 27.7 K
2 | backbone.layers.0 | Linear            | 23.1 K
3 | backbone.layers.1 | ReLU              | 0     
4 | backbone.layers.2 | Linear            | 4.2 K 
5 | backbone.layers.3 | Sequential        | 0     
6 | backbone.layers.4 | Linear            | 390   
7 | backbone.layers.5 | Softmax           | 0     
8 | fc                | Identity          | 0     
9 | loss_fn           | CrossEntropyLoss  | 0     
--------------------------------------------------------
27.7 K    Trainable params
0         Non-trainable params
27.7 K    Total params
0.111     Total estimated model params size (MB)

Traceback (most recent call last):
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/har_classification/tnc_head_classifier.py", line 131, in <module>
    auto_main(options)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/cli.py", line 52, in auto_main
    return pipeline.run()
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/mlflow_train.py", line 180, in run
    trainer.fit(model, datamodule)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 543, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 579, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 986, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1030, in _run_stage
    self._run_sanity_check()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1059, in _run_sanity_check
    val_loop.run()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/utilities.py", line 182, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/evaluation_loop.py", line 135, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/evaluation_loop.py", line 396, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 412, in validation_step
    return self.lightning_module.validation_step(*args, **kwargs)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 72, in validation_step
    return self.single_step(batch, batch_idx, "val")
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 55, in single_step
    y_hat = self.forward(x)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 35, in forward
    x = self.backbone(x)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/vscode/.local/lib/python3.10/site-packages/lightly/models/modules/heads.py", line 65, in forward
    projection: Tensor = self.layers(x)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (128x180 and 360x64)
 ************ Training tnchead_fft on RealWorld_thigh ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name              | Type              | Params
--------------------------------------------------------
0 | backbone          | TNCPredictionHead | 27.7 K
1 | backbone.layers   | Sequential        | 27.7 K
2 | backbone.layers.0 | Linear            | 23.1 K
3 | backbone.layers.1 | ReLU              | 0     
4 | backbone.layers.2 | Linear            | 4.2 K 
5 | backbone.layers.3 | Sequential        | 0     
6 | backbone.layers.4 | Linear            | 390   
7 | backbone.layers.5 | Softmax           | 0     
8 | fc                | Identity          | 0     
9 | loss_fn           | CrossEntropyLoss  | 0     
--------------------------------------------------------
27.7 K    Trainable params
0         Non-trainable params
27.7 K    Total params
0.111     Total estimated model params size (MB)

Traceback (most recent call last):
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/har_classification/tnc_head_classifier.py", line 131, in <module>
    auto_main(options)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/cli.py", line 52, in auto_main
    return pipeline.run()
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/mlflow_train.py", line 180, in run
    trainer.fit(model, datamodule)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 543, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 579, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 986, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1030, in _run_stage
    self._run_sanity_check()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1059, in _run_sanity_check
    val_loop.run()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/utilities.py", line 182, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/evaluation_loop.py", line 135, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/evaluation_loop.py", line 396, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 412, in validation_step
    return self.lightning_module.validation_step(*args, **kwargs)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 72, in validation_step
    return self.single_step(batch, batch_idx, "val")
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 55, in single_step
    y_hat = self.forward(x)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 35, in forward
    x = self.backbone(x)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/vscode/.local/lib/python3.10/site-packages/lightly/models/modules/heads.py", line 65, in forward
    projection: Tensor = self.layers(x)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (128x180 and 360x64)
 ************ Training tnchead_fft on RealWorld_waist ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name              | Type              | Params
--------------------------------------------------------
0 | backbone          | TNCPredictionHead | 27.7 K
1 | backbone.layers   | Sequential        | 27.7 K
2 | backbone.layers.0 | Linear            | 23.1 K
3 | backbone.layers.1 | ReLU              | 0     
4 | backbone.layers.2 | Linear            | 4.2 K 
5 | backbone.layers.3 | Sequential        | 0     
6 | backbone.layers.4 | Linear            | 390   
7 | backbone.layers.5 | Softmax           | 0     
8 | fc                | Identity          | 0     
9 | loss_fn           | CrossEntropyLoss  | 0     
--------------------------------------------------------
27.7 K    Trainable params
0         Non-trainable params
27.7 K    Total params
0.111     Total estimated model params size (MB)

Traceback (most recent call last):
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/har_classification/tnc_head_classifier.py", line 131, in <module>
    auto_main(options)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/cli.py", line 52, in auto_main
    return pipeline.run()
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/mlflow_train.py", line 180, in run
    trainer.fit(model, datamodule)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 543, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 579, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 986, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1030, in _run_stage
    self._run_sanity_check()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1059, in _run_sanity_check
    val_loop.run()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/utilities.py", line 182, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/evaluation_loop.py", line 135, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/evaluation_loop.py", line 396, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 412, in validation_step
    return self.lightning_module.validation_step(*args, **kwargs)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 72, in validation_step
    return self.single_step(batch, batch_idx, "val")
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 55, in single_step
    y_hat = self.forward(x)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 35, in forward
    x = self.backbone(x)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/vscode/.local/lib/python3.10/site-packages/lightly/models/modules/heads.py", line 65, in forward
    projection: Tensor = self.layers(x)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (128x180 and 360x64)
 ************ Training tnchead_fft on UCI ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name              | Type              | Params
--------------------------------------------------------
0 | backbone          | TNCPredictionHead | 27.7 K
1 | backbone.layers   | Sequential        | 27.7 K
2 | backbone.layers.0 | Linear            | 23.1 K
3 | backbone.layers.1 | ReLU              | 0     
4 | backbone.layers.2 | Linear            | 4.2 K 
5 | backbone.layers.3 | Sequential        | 0     
6 | backbone.layers.4 | Linear            | 390   
7 | backbone.layers.5 | Softmax           | 0     
8 | fc                | Identity          | 0     
9 | loss_fn           | CrossEntropyLoss  | 0     
--------------------------------------------------------
27.7 K    Trainable params
0         Non-trainable params
27.7 K    Total params
0.111     Total estimated model params size (MB)

Traceback (most recent call last):
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/har_classification/tnc_head_classifier.py", line 131, in <module>
    auto_main(options)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/cli.py", line 52, in auto_main
    return pipeline.run()
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/mlflow_train.py", line 180, in run
    trainer.fit(model, datamodule)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 543, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 579, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 986, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1030, in _run_stage
    self._run_sanity_check()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1059, in _run_sanity_check
    val_loop.run()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/utilities.py", line 182, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/evaluation_loop.py", line 135, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/evaluation_loop.py", line 396, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 412, in validation_step
    return self.lightning_module.validation_step(*args, **kwargs)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 72, in validation_step
    return self.single_step(batch, batch_idx, "val")
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 55, in single_step
    y_hat = self.forward(x)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 35, in forward
    x = self.backbone(x)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/vscode/.local/lib/python3.10/site-packages/lightly/models/modules/heads.py", line 65, in forward
    projection: Tensor = self.layers(x)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (128x180 and 360x64)
 ************ Training tnchead_fft on WISDM ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name              | Type              | Params
--------------------------------------------------------
0 | backbone          | TNCPredictionHead | 27.7 K
1 | backbone.layers   | Sequential        | 27.7 K
2 | backbone.layers.0 | Linear            | 23.1 K
3 | backbone.layers.1 | ReLU              | 0     
4 | backbone.layers.2 | Linear            | 4.2 K 
5 | backbone.layers.3 | Sequential        | 0     
6 | backbone.layers.4 | Linear            | 390   
7 | backbone.layers.5 | Softmax           | 0     
8 | fc                | Identity          | 0     
9 | loss_fn           | CrossEntropyLoss  | 0     
--------------------------------------------------------
27.7 K    Trainable params
0         Non-trainable params
27.7 K    Total params
0.111     Total estimated model params size (MB)

Traceback (most recent call last):
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/har_classification/tnc_head_classifier.py", line 131, in <module>
    auto_main(options)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/cli.py", line 52, in auto_main
    return pipeline.run()
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/mlflow_train.py", line 180, in run
    trainer.fit(model, datamodule)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 543, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 579, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 986, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1030, in _run_stage
    self._run_sanity_check()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1059, in _run_sanity_check
    val_loop.run()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/utilities.py", line 182, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/evaluation_loop.py", line 135, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/evaluation_loop.py", line 396, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 412, in validation_step
    return self.lightning_module.validation_step(*args, **kwargs)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 72, in validation_step
    return self.single_step(batch, batch_idx, "val")
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 55, in single_step
    y_hat = self.forward(x)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 35, in forward
    x = self.backbone(x)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/vscode/.local/lib/python3.10/site-packages/lightly/models/modules/heads.py", line 65, in forward
    projection: Tensor = self.layers(x)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (128x180 and 360x64)
 ************ Training mlp3x64_fft on KuHar ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name           | Type             | Params
-----------------------------------------------------
0  | backbone       | Sequential       | 56.2 K
1  | backbone.fc1   | Linear           | 23.2 K
2  | backbone.relu1 | ReLU             | 0     
3  | backbone.fc2   | Linear           | 16.5 K
4  | backbone.relu2 | ReLU             | 0     
5  | backbone.fc3   | Linear           | 16.5 K
6  | backbone.relu3 | ReLU             | 0     
7  | fc             | Sequential       | 774   
8  | fc.0           | Linear           | 774   
9  | fc.1           | Softmax          | 0     
10 | loss_fn        | CrossEntropyLoss | 0     
-----------------------------------------------------
57.0 K    Trainable params
0         Non-trainable params
57.0 K    Total params
0.228     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 150.130 seconds
Epoch 65/299 ━━━━━━━━━━━━━━━━━━ 11/11 0:00:01 • 0:00:00 292.60it/s v_num: d688         
                                                                   val_loss: 1.346     
                                                                   train_loss: 1.052   
 ************ Training mlp3x64_fft on MotionSense ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name           | Type             | Params
-----------------------------------------------------
0  | backbone       | Sequential       | 56.2 K
1  | backbone.fc1   | Linear           | 23.2 K
2  | backbone.relu1 | ReLU             | 0     
3  | backbone.fc2   | Linear           | 16.5 K
4  | backbone.relu2 | ReLU             | 0     
5  | backbone.fc3   | Linear           | 16.5 K
6  | backbone.relu3 | ReLU             | 0     
7  | fc             | Sequential       | 774   
8  | fc.0           | Linear           | 774   
9  | fc.1           | Softmax          | 0     
10 | loss_fn        | CrossEntropyLoss | 0     
-----------------------------------------------------
57.0 K    Trainable params
0         Non-trainable params
57.0 K    Total params
0.228     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (28) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 172.362 seconds
Epoch 73/299 ━━━━━━━━━━━━━━━━━━ 28/28 0:00:01 • 0:00:00 224.34it/s v_num: 1708         
                                                                   val_loss: 1.240     
                                                                   train_loss: 1.059   
 ************ Training mlp3x64_fft on RealWorld_thigh ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name           | Type             | Params
-----------------------------------------------------
0  | backbone       | Sequential       | 56.2 K
1  | backbone.fc1   | Linear           | 23.2 K
2  | backbone.relu1 | ReLU             | 0     
3  | backbone.fc2   | Linear           | 16.5 K
4  | backbone.relu2 | ReLU             | 0     
5  | backbone.fc3   | Linear           | 16.5 K
6  | backbone.relu3 | ReLU             | 0     
7  | fc             | Sequential       | 774   
8  | fc.0           | Linear           | 774   
9  | fc.1           | Softmax          | 0     
10 | loss_fn        | CrossEntropyLoss | 0     
-----------------------------------------------------
57.0 K    Trainable params
0         Non-trainable params
57.0 K    Total params
0.228     Total estimated model params size (MB)
--> Overall fit time: 109.473 seconds
Epoch 43/299 ━━━━━━━━━━━━━━━ 66/66 0:00:01 •       320.45it/s v_num: 5ded    
                                   0:00:00                    val_loss: 1.392
                                                              train_loss:    
                                                              1.094          
 ************ Training mlp3x64_fft on RealWorld_waist ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name           | Type             | Params
-----------------------------------------------------
0  | backbone       | Sequential       | 56.2 K
1  | backbone.fc1   | Linear           | 23.2 K
2  | backbone.relu1 | ReLU             | 0     
3  | backbone.fc2   | Linear           | 16.5 K
4  | backbone.relu2 | ReLU             | 0     
5  | backbone.fc3   | Linear           | 16.5 K
6  | backbone.relu3 | ReLU             | 0     
7  | fc             | Sequential       | 774   
8  | fc.0           | Linear           | 774   
9  | fc.1           | Softmax          | 0     
10 | loss_fn        | CrossEntropyLoss | 0     
-----------------------------------------------------
57.0 K    Trainable params
0         Non-trainable params
57.0 K    Total params
0.228     Total estimated model params size (MB)
--> Overall fit time: 161.231 seconds
Epoch 62/299 ━━━━━━━━━━━━━━━ 81/81 0:00:01 •       278.80it/s v_num: 5699    
                                   0:00:00                    val_loss: 1.265
                                                              train_loss:    
                                                              1.105          
 ************ Training mlp3x64_fft on UCI ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name           | Type             | Params
-----------------------------------------------------
0  | backbone       | Sequential       | 56.2 K
1  | backbone.fc1   | Linear           | 23.2 K
2  | backbone.relu1 | ReLU             | 0     
3  | backbone.fc2   | Linear           | 16.5 K
4  | backbone.relu2 | ReLU             | 0     
5  | backbone.fc3   | Linear           | 16.5 K
6  | backbone.relu3 | ReLU             | 0     
7  | fc             | Sequential       | 774   
8  | fc.0           | Linear           | 774   
9  | fc.1           | Softmax          | 0     
10 | loss_fn        | CrossEntropyLoss | 0     
-----------------------------------------------------
57.0 K    Trainable params
0         Non-trainable params
57.0 K    Total params
0.228     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (19) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 147.218 seconds
Epoch 65/299 ━━━━━━━━━━━━━━━ 19/19 0:00:01 •       342.78it/s v_num: 5a86    
                                   0:00:00                    val_loss: 1.195
                                                              train_loss:    
                                                              1.069          
 ************ Training mlp3x64_fft on WISDM ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name           | Type             | Params
-----------------------------------------------------
0  | backbone       | Sequential       | 56.2 K
1  | backbone.fc1   | Linear           | 23.2 K
2  | backbone.relu1 | ReLU             | 0     
3  | backbone.fc2   | Linear           | 16.5 K
4  | backbone.relu2 | ReLU             | 0     
5  | backbone.fc3   | Linear           | 16.5 K
6  | backbone.relu3 | ReLU             | 0     
7  | fc             | Sequential       | 774   
8  | fc.0           | Linear           | 774   
9  | fc.1           | Softmax          | 0     
10 | loss_fn        | CrossEntropyLoss | 0     
-----------------------------------------------------
57.0 K    Trainable params
0         Non-trainable params
57.0 K    Total params
0.228     Total estimated model params size (MB)
Epoch 0/299                  0/86 0:00:00 •        0.00it/s v_num: 5c6b      
                                  -:--:--                   val_loss: 1.912  
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 579, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 986, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1032, in _run_stage
    self.fit_loop.run()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 138, in run
    self.advance(data_fetcher)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 242, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 191, in run
    self._optimizer_step(batch_idx, closure)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 269, in _optimizer_step
    call._call_lightning_module_hook(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 157, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/module.py", line 1303, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/optimizer.py", line 152, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 33, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py", line 123, in step
    loss = closure()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 108, in _wrap_closure
    closure_result = closure()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 144, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 129, in closure
    step_output = self._step_fn()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 391, in training_step
    return self.lightning_module.training_step(*args, **kwargs)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 69, in training_step
    return self.single_step(batch, batch_idx, "train")
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 57, in single_step
    self.log(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/module.py", line 516, in log
    results.log(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 406, in log
    self.register_key(key, meta, value)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 423, in register_key
    metric = _ResultMetric(meta, isinstance(value, Tensor)).to(value.device)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 311, in to
    self.__dict__.update(apply_to_collection(d, (Tensor, Metric), move_data_to_device, *args, **kwargs))
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 72, in apply_to_collection
    return _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 104, in _apply_to_collection_slow
    v = _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 104, in _apply_to_collection_slow
    v = _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 96, in _apply_to_collection_slow
    return function(data, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/apply_func.py", line 102, in move_data_to_device
    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 64, in apply_to_collection
    return function(data, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/apply_func.py", line 96, in batch_to
    data_output = data.to(device, **kwargs)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/har_classification/mlp.py", line 132, in <module>
    auto_main(options)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/cli.py", line 52, in auto_main
    return pipeline.run()
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/mlflow_train.py", line 180, in run
    trainer.fit(model, datamodule)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 543, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 68, in _call_and_handle_interrupt
    trainer._teardown()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1009, in _teardown
    self.strategy.teardown()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 537, in teardown
    self.lightning_module.cpu()
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/device_dtype_mixin.py", line 82, in cpu
    return super().cpu()
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 954, in cpu
    return self._apply(lambda t: t.cpu())
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 820, in _apply
    param_applied = fn(param)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 954, in <lambda>
    return self._apply(lambda t: t.cpu())
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [31,0,0] Assertion `t >= 0 && t < n_classes` failed.
Exception in thread Thread-2 (_pin_memory_loop):
Traceback (most recent call last):
  File "/usr/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py", line 54, in _pin_memory_loop
    do_one_step()
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py", line 31, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/usr/lib/python3.10/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/reductions.py", line 316, in rebuild_storage_fd
    fd = df.detach()
  File "/usr/lib/python3.10/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/usr/lib/python3.10/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 509, in Client
    deliver_challenge(c, authkey)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 740, in deliver_challenge
    response = connection.recv_bytes(256)        # reject large message
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer
 ************ Training simple2Dconv_fft on KuHar ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name       | Type             | Params
-------------------------------------------------
0  | backbone   | Sequential       | 13.5 K
1  | backbone.0 | Conv2d           | 1.2 K 
2  | backbone.1 | ReLU             | 0     
3  | backbone.2 | MaxPool2d        | 0     
4  | backbone.3 | Conv2d           | 12.4 K
5  | backbone.4 | ReLU             | 0     
6  | backbone.5 | MaxPool2d        | 0     
7  | fc         | Sequential       | 696 K 
8  | fc.0       | Linear           | 193 K 
9  | fc.1       | ReLU             | 0     
10 | fc.2       | Linear           | 500 K 
11 | fc.3       | ReLU             | 0     
12 | fc.4       | Linear           | 3.0 K 
13 | loss_fn    | CrossEntropyLoss | 0     
-------------------------------------------------
710 K     Trainable params
0         Non-trainable params
710 K     Total params
2.840     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 79.415 seconds
Epoch 32/299 ━━━━━━━━━━━━━━━ 11/11 0:00:01 •       207.45it/s v_num: f75a    
                                   0:00:00                    val_loss:      
                                                              33.674         
                                                              train_loss:    
                                                              0.011          
 ************ Training simple2Dconv_fft on MotionSense ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name       | Type             | Params
-------------------------------------------------
0  | backbone   | Sequential       | 13.5 K
1  | backbone.0 | Conv2d           | 1.2 K 
2  | backbone.1 | ReLU             | 0     
3  | backbone.2 | MaxPool2d        | 0     
4  | backbone.3 | Conv2d           | 12.4 K
5  | backbone.4 | ReLU             | 0     
6  | backbone.5 | MaxPool2d        | 0     
7  | fc         | Sequential       | 696 K 
8  | fc.0       | Linear           | 193 K 
9  | fc.1       | ReLU             | 0     
10 | fc.2       | Linear           | 500 K 
11 | fc.3       | ReLU             | 0     
12 | fc.4       | Linear           | 3.0 K 
13 | loss_fn    | CrossEntropyLoss | 0     
-------------------------------------------------
710 K     Trainable params
0         Non-trainable params
710 K     Total params
2.840     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (28) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 80.202 seconds
Epoch 31/299 ━━━━━━━━━━━━━━━ 28/28 0:00:01 •       273.82it/s v_num: 21d9    
                                   0:00:00                    val_loss: 5.849
                                                              train_loss:    
                                                              0.024          
 ************ Training simple2Dconv_fft on RealWorld_thigh ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name       | Type             | Params
-------------------------------------------------
0  | backbone   | Sequential       | 13.5 K
1  | backbone.0 | Conv2d           | 1.2 K 
2  | backbone.1 | ReLU             | 0     
3  | backbone.2 | MaxPool2d        | 0     
4  | backbone.3 | Conv2d           | 12.4 K
5  | backbone.4 | ReLU             | 0     
6  | backbone.5 | MaxPool2d        | 0     
7  | fc         | Sequential       | 696 K 
8  | fc.0       | Linear           | 193 K 
9  | fc.1       | ReLU             | 0     
10 | fc.2       | Linear           | 500 K 
11 | fc.3       | ReLU             | 0     
12 | fc.4       | Linear           | 3.0 K 
13 | loss_fn    | CrossEntropyLoss | 0     
-------------------------------------------------
710 K     Trainable params
0         Non-trainable params
710 K     Total params
2.840     Total estimated model params size (MB)
--> Overall fit time: 86.318 seconds
Epoch 31/299 ━━━━━━━━━━━━━━━ 66/66 0:00:01 •       268.60it/s v_num: e939    
                                   0:00:00                    val_loss: 4.710
                                                              train_loss:    
                                                              0.003          
 ************ Training simple2Dconv_fft on RealWorld_waist ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name       | Type             | Params
-------------------------------------------------
0  | backbone   | Sequential       | 13.5 K
1  | backbone.0 | Conv2d           | 1.2 K 
2  | backbone.1 | ReLU             | 0     
3  | backbone.2 | MaxPool2d        | 0     
4  | backbone.3 | Conv2d           | 12.4 K
5  | backbone.4 | ReLU             | 0     
6  | backbone.5 | MaxPool2d        | 0     
7  | fc         | Sequential       | 696 K 
8  | fc.0       | Linear           | 193 K 
9  | fc.1       | ReLU             | 0     
10 | fc.2       | Linear           | 500 K 
11 | fc.3       | ReLU             | 0     
12 | fc.4       | Linear           | 3.0 K 
13 | loss_fn    | CrossEntropyLoss | 0     
-------------------------------------------------
710 K     Trainable params
0         Non-trainable params
710 K     Total params
2.840     Total estimated model params size (MB)
--> Overall fit time: 96.654 seconds
Epoch 33/299 ━━━━━━━━━━━━━━━ 81/81 0:00:01 •       247.29it/s v_num: 9ec0    
                                   0:00:00                    val_loss: 2.021
                                                              train_loss:    
                                                              0.017          
 ************ Training simple2Dconv_fft on UCI ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name       | Type             | Params
-------------------------------------------------
0  | backbone   | Sequential       | 13.5 K
1  | backbone.0 | Conv2d           | 1.2 K 
2  | backbone.1 | ReLU             | 0     
3  | backbone.2 | MaxPool2d        | 0     
4  | backbone.3 | Conv2d           | 12.4 K
5  | backbone.4 | ReLU             | 0     
6  | backbone.5 | MaxPool2d        | 0     
7  | fc         | Sequential       | 696 K 
8  | fc.0       | Linear           | 193 K 
9  | fc.1       | ReLU             | 0     
10 | fc.2       | Linear           | 500 K 
11 | fc.3       | ReLU             | 0     
12 | fc.4       | Linear           | 3.0 K 
13 | loss_fn    | CrossEntropyLoss | 0     
-------------------------------------------------
710 K     Trainable params
0         Non-trainable params
710 K     Total params
2.840     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (19) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 88.407 seconds
Epoch 34/299 ━━━━━━━━━━━━━━━ 19/19 0:00:01 •       292.46it/s v_num: 8fdb    
                                   0:00:00                    val_loss: 1.086
                                                              train_loss:    
                                                              0.004          
 ************ Training simple2Dconv_fft on WISDM ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name       | Type             | Params
-------------------------------------------------
0  | backbone   | Sequential       | 13.5 K
1  | backbone.0 | Conv2d           | 1.2 K 
2  | backbone.1 | ReLU             | 0     
3  | backbone.2 | MaxPool2d        | 0     
4  | backbone.3 | Conv2d           | 12.4 K
5  | backbone.4 | ReLU             | 0     
6  | backbone.5 | MaxPool2d        | 0     
7  | fc         | Sequential       | 696 K 
8  | fc.0       | Linear           | 193 K 
9  | fc.1       | ReLU             | 0     
10 | fc.2       | Linear           | 500 K 
11 | fc.3       | ReLU             | 0     
12 | fc.4       | Linear           | 3.0 K 
13 | loss_fn    | CrossEntropyLoss | 0     
-------------------------------------------------
710 K     Trainable params
0         Non-trainable params
710 K     Total params
2.840     Total estimated model params size (MB)
Epoch 0/299                  0/86 0:00:00 •        0.00it/s v_num: bf48      
                                  -:--:--                   val_loss: 2.234  
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 579, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 986, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1032, in _run_stage
    self.fit_loop.run()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 138, in run
    self.advance(data_fetcher)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 242, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 191, in run
    self._optimizer_step(batch_idx, closure)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 269, in _optimizer_step
    call._call_lightning_module_hook(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 157, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/module.py", line 1303, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/optimizer.py", line 152, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 33, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py", line 123, in step
    loss = closure()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 108, in _wrap_closure
    closure_result = closure()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 144, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 129, in closure
    step_output = self._step_fn()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 391, in training_step
    return self.lightning_module.training_step(*args, **kwargs)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 69, in training_step
    return self.single_step(batch, batch_idx, "train")
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 57, in single_step
    self.log(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/module.py", line 516, in log
    results.log(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 406, in log
    self.register_key(key, meta, value)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 423, in register_key
    metric = _ResultMetric(meta, isinstance(value, Tensor)).to(value.device)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 311, in to
    self.__dict__.update(apply_to_collection(d, (Tensor, Metric), move_data_to_device, *args, **kwargs))
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 72, in apply_to_collection
    return _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 104, in _apply_to_collection_slow
    v = _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 104, in _apply_to_collection_slow
    v = _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 96, in _apply_to_collection_slow
    return function(data, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/apply_func.py", line 102, in move_data_to_device
    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 64, in apply_to_collection
    return function(data, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/apply_func.py", line 96, in batch_to
    data_output = data.to(device, **kwargs)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/har_classification/simple2Dconv_classifier.py", line 135, in <module>
    auto_main(options)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/cli.py", line 52, in auto_main
    return pipeline.run()
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/mlflow_train.py", line 180, in run
    trainer.fit(model, datamodule)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 543, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 68, in _call_and_handle_interrupt
    trainer._teardown()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1009, in _teardown
    self.strategy.teardown()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 537, in teardown
    self.lightning_module.cpu()
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/device_dtype_mixin.py", line 82, in cpu
    return super().cpu()
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 954, in cpu
    return self._apply(lambda t: t.cpu())
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 820, in _apply
    param_applied = fn(param)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 954, in <lambda>
    return self._apply(lambda t: t.cpu())
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [13,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [20,0,0] Assertion `t >= 0 && t < n_classes` failed.
Exception in thread Thread-2 (_pin_memory_loop):
Traceback (most recent call last):
  File "/usr/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py", line 54, in _pin_memory_loop
    do_one_step()
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py", line 31, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/usr/lib/python3.10/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/reductions.py", line 316, in rebuild_storage_fd
    fd = df.detach()
  File "/usr/lib/python3.10/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/usr/lib/python3.10/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 509, in Client
    deliver_challenge(c, authkey)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 740, in deliver_challenge
    response = connection.recv_bytes(256)        # reject large message
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer
 ************ Training mlp3x64 on KuHar ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name           | Type             | Params
-----------------------------------------------------
0  | backbone       | Sequential       | 79.2 K
1  | backbone.fc1   | Linear           | 46.2 K
2  | backbone.relu1 | ReLU             | 0     
3  | backbone.fc2   | Linear           | 16.5 K
4  | backbone.relu2 | ReLU             | 0     
5  | backbone.fc3   | Linear           | 16.5 K
6  | backbone.relu3 | ReLU             | 0     
7  | fc             | Sequential       | 774   
8  | fc.0           | Linear           | 774   
9  | fc.1           | Softmax          | 0     
10 | loss_fn        | CrossEntropyLoss | 0     
-----------------------------------------------------
80.0 K    Trainable params
0         Non-trainable params
80.0 K    Total params
0.320     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 163.817 seconds
Epoch 73/299 ━━━━━━━━━━━━━━━ 11/11 0:00:01 •       296.59it/s v_num: 0b63    
                                   0:00:00                    val_loss: 1.526
                                                              train_loss:    
                                                              1.058          
 ************ Training mlp3x64 on MotionSense ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name           | Type             | Params
-----------------------------------------------------
0  | backbone       | Sequential       | 79.2 K
1  | backbone.fc1   | Linear           | 46.2 K
2  | backbone.relu1 | ReLU             | 0     
3  | backbone.fc2   | Linear           | 16.5 K
4  | backbone.relu2 | ReLU             | 0     
5  | backbone.fc3   | Linear           | 16.5 K
6  | backbone.relu3 | ReLU             | 0     
7  | fc             | Sequential       | 774   
8  | fc.0           | Linear           | 774   
9  | fc.1           | Softmax          | 0     
10 | loss_fn        | CrossEntropyLoss | 0     
-----------------------------------------------------
80.0 K    Trainable params
0         Non-trainable params
80.0 K    Total params
0.320     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (28) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 124.013 seconds
Epoch 53/299 ━━━━━━━━━━━━━━━ 28/28 0:00:01 •       299.55it/s v_num: 677b    
                                   0:00:00                    val_loss: 1.326
                                                              train_loss:    
                                                              1.095          
 ************ Training mlp3x64 on RealWorld_thigh ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name           | Type             | Params
-----------------------------------------------------
0  | backbone       | Sequential       | 79.2 K
1  | backbone.fc1   | Linear           | 46.2 K
2  | backbone.relu1 | ReLU             | 0     
3  | backbone.fc2   | Linear           | 16.5 K
4  | backbone.relu2 | ReLU             | 0     
5  | backbone.fc3   | Linear           | 16.5 K
6  | backbone.relu3 | ReLU             | 0     
7  | fc             | Sequential       | 774   
8  | fc.0           | Linear           | 774   
9  | fc.1           | Softmax          | 0     
10 | loss_fn        | CrossEntropyLoss | 0     
-----------------------------------------------------
80.0 K    Trainable params
0         Non-trainable params
80.0 K    Total params
0.320     Total estimated model params size (MB)
--> Overall fit time: 78.718 seconds
Epoch 31/299 ━━━━━━━━━━━━━━━ 66/66 0:00:01 •       325.35it/s v_num: 8aa3    
                                   0:00:00                    val_loss: 1.476
                                                              train_loss:    
                                                              1.146          
 ************ Training mlp3x64 on RealWorld_waist ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name           | Type             | Params
-----------------------------------------------------
0  | backbone       | Sequential       | 79.2 K
1  | backbone.fc1   | Linear           | 46.2 K
2  | backbone.relu1 | ReLU             | 0     
3  | backbone.fc2   | Linear           | 16.5 K
4  | backbone.relu2 | ReLU             | 0     
5  | backbone.fc3   | Linear           | 16.5 K
6  | backbone.relu3 | ReLU             | 0     
7  | fc             | Sequential       | 774   
8  | fc.0           | Linear           | 774   
9  | fc.1           | Softmax          | 0     
10 | loss_fn        | CrossEntropyLoss | 0     
-----------------------------------------------------
80.0 K    Trainable params
0         Non-trainable params
80.0 K    Total params
0.320     Total estimated model params size (MB)
--> Overall fit time: 159.872 seconds
Epoch 62/299 ━━━━━━━━━━━━━━━ 81/81 0:00:01 •       334.52it/s v_num: c086    
                                   0:00:00                    val_loss: 1.258
                                                              train_loss:    
                                                              1.112          
 ************ Training mlp3x64 on UCI ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name           | Type             | Params
-----------------------------------------------------
0  | backbone       | Sequential       | 79.2 K
1  | backbone.fc1   | Linear           | 46.2 K
2  | backbone.relu1 | ReLU             | 0     
3  | backbone.fc2   | Linear           | 16.5 K
4  | backbone.relu2 | ReLU             | 0     
5  | backbone.fc3   | Linear           | 16.5 K
6  | backbone.relu3 | ReLU             | 0     
7  | fc             | Sequential       | 774   
8  | fc.0           | Linear           | 774   
9  | fc.1           | Softmax          | 0     
10 | loss_fn        | CrossEntropyLoss | 0     
-----------------------------------------------------
80.0 K    Trainable params
0         Non-trainable params
80.0 K    Total params
0.320     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (19) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 212.175 seconds
Epoch 68/299 ━━━━━━━━━━━━━━━ 19/19 0:00:01 •       249.12it/s v_num: 1a9c    
                                   0:00:00                    val_loss: 1.240
                                                              train_loss:    
                                                              1.065          
 ************ Training mlp3x64 on WISDM ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name           | Type             | Params
-----------------------------------------------------
0  | backbone       | Sequential       | 79.2 K
1  | backbone.fc1   | Linear           | 46.2 K
2  | backbone.relu1 | ReLU             | 0     
3  | backbone.fc2   | Linear           | 16.5 K
4  | backbone.relu2 | ReLU             | 0     
5  | backbone.fc3   | Linear           | 16.5 K
6  | backbone.relu3 | ReLU             | 0     
7  | fc             | Sequential       | 774   
8  | fc.0           | Linear           | 774   
9  | fc.1           | Softmax          | 0     
10 | loss_fn        | CrossEntropyLoss | 0     
-----------------------------------------------------
80.0 K    Trainable params
0         Non-trainable params
80.0 K    Total params
0.320     Total estimated model params size (MB)
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [25,0,0] Assertion `t >= 0 && t < n_classes` failed.
Epoch 0/299                  0/86 0:00:00 •        0.00it/s v_num: dc6c      
                                  -:--:--                   val_loss: 1.782  
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 579, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 986, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1032, in _run_stage
    self.fit_loop.run()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 138, in run
    self.advance(data_fetcher)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 242, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 191, in run
    self._optimizer_step(batch_idx, closure)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 269, in _optimizer_step
    call._call_lightning_module_hook(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 157, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/module.py", line 1303, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/optimizer.py", line 152, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 33, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py", line 123, in step
    loss = closure()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 108, in _wrap_closure
    closure_result = closure()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 144, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 129, in closure
    step_output = self._step_fn()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 391, in training_step
    return self.lightning_module.training_step(*args, **kwargs)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 69, in training_step
    return self.single_step(batch, batch_idx, "train")
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 57, in single_step
    self.log(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/module.py", line 516, in log
    results.log(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 406, in log
    self.register_key(key, meta, value)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 423, in register_key
    metric = _ResultMetric(meta, isinstance(value, Tensor)).to(value.device)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 311, in to
    self.__dict__.update(apply_to_collection(d, (Tensor, Metric), move_data_to_device, *args, **kwargs))
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 72, in apply_to_collection
    return _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 104, in _apply_to_collection_slow
    v = _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 104, in _apply_to_collection_slow
    v = _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 96, in _apply_to_collection_slow
    return function(data, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/apply_func.py", line 102, in move_data_to_device
    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 64, in apply_to_collection
    return function(data, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/apply_func.py", line 96, in batch_to
    data_output = data.to(device, **kwargs)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/har_classification/mlp.py", line 132, in <module>
    auto_main(options)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/cli.py", line 52, in auto_main
    return pipeline.run()
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/mlflow_train.py", line 180, in run
    trainer.fit(model, datamodule)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 543, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 68, in _call_and_handle_interrupt
    trainer._teardown()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1009, in _teardown
    self.strategy.teardown()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 537, in teardown
    self.lightning_module.cpu()
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/device_dtype_mixin.py", line 82, in cpu
    return super().cpu()
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 954, in cpu
    return self._apply(lambda t: t.cpu())
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 820, in _apply
    param_applied = fn(param)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 954, in <lambda>
    return self._apply(lambda t: t.cpu())
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception in thread Thread-2 (_pin_memory_loop):
Traceback (most recent call last):
  File "/usr/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py", line 54, in _pin_memory_loop
    do_one_step()
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py", line 31, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/usr/lib/python3.10/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/reductions.py", line 316, in rebuild_storage_fd
    fd = df.detach()
  File "/usr/lib/python3.10/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/usr/lib/python3.10/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 508, in Client
    answer_challenge(c, authkey)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 752, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer
 ************ Training mlp1x64 on KuHar ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name           | Type             | Params
----------------------------------------------------
0 | backbone       | Sequential       | 46.2 K
1 | backbone.fc1   | Linear           | 46.2 K
2 | backbone.relu1 | ReLU             | 0     
3 | fc             | Sequential       | 774   
4 | fc.0           | Linear           | 774   
5 | fc.1           | Softmax          | 0     
6 | loss_fn        | CrossEntropyLoss | 0     
----------------------------------------------------
47.0 K    Trainable params
0         Non-trainable params
47.0 K    Total params
0.188     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 146.532 seconds
Epoch 67/299 ━━━━━━━━━━━━━━━ 11/11 0:00:01 •       341.43it/s v_num: 93b1    
                                   0:00:00                    val_loss: 1.659
                                                              train_loss:    
                                                              1.189          
 ************ Training mlp1x64 on MotionSense ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name           | Type             | Params
----------------------------------------------------
0 | backbone       | Sequential       | 46.2 K
1 | backbone.fc1   | Linear           | 46.2 K
2 | backbone.relu1 | ReLU             | 0     
3 | fc             | Sequential       | 774   
4 | fc.0           | Linear           | 774   
5 | fc.1           | Softmax          | 0     
6 | loss_fn        | CrossEntropyLoss | 0     
----------------------------------------------------
47.0 K    Trainable params
0         Non-trainable params
47.0 K    Total params
0.188     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (28) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 235.819 seconds
Epoch 102/299 ━━━━━━━━━━━━━━ 28/28 0:00:01 •       424.58it/s v_num: a789    
                                   0:00:00                    val_loss: 1.355
                                                              train_loss:    
                                                              1.080          
 ************ Training mlp1x64 on RealWorld_thigh ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name           | Type             | Params
----------------------------------------------------
0 | backbone       | Sequential       | 46.2 K
1 | backbone.fc1   | Linear           | 46.2 K
2 | backbone.relu1 | ReLU             | 0     
3 | fc             | Sequential       | 774   
4 | fc.0           | Linear           | 774   
5 | fc.1           | Softmax          | 0     
6 | loss_fn        | CrossEntropyLoss | 0     
----------------------------------------------------
47.0 K    Trainable params
0         Non-trainable params
47.0 K    Total params
0.188     Total estimated model params size (MB)
--> Overall fit time: 81.950 seconds
Epoch 33/299 ━━━━━━━━━━━━━━━ 66/66 0:00:01 •       417.40it/s v_num: 707d    
                                   0:00:00                    val_loss: 1.458
                                                              train_loss:    
                                                              1.157          
 ************ Training mlp1x64 on RealWorld_waist ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name           | Type             | Params
----------------------------------------------------
0 | backbone       | Sequential       | 46.2 K
1 | backbone.fc1   | Linear           | 46.2 K
2 | backbone.relu1 | ReLU             | 0     
3 | fc             | Sequential       | 774   
4 | fc.0           | Linear           | 774   
5 | fc.1           | Softmax          | 0     
6 | loss_fn        | CrossEntropyLoss | 0     
----------------------------------------------------
47.0 K    Trainable params
0         Non-trainable params
47.0 K    Total params
0.188     Total estimated model params size (MB)
--> Overall fit time: 174.214 seconds
Epoch 67/299 ━━━━━━━━━━━━━━━ 81/81 0:00:01 •       384.56it/s v_num: 09d4    
                                   0:00:00                    val_loss: 1.297
                                                              train_loss:    
                                                              1.095          
 ************ Training mlp1x64 on UCI ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name           | Type             | Params
----------------------------------------------------
0 | backbone       | Sequential       | 46.2 K
1 | backbone.fc1   | Linear           | 46.2 K
2 | backbone.relu1 | ReLU             | 0     
3 | fc             | Sequential       | 774   
4 | fc.0           | Linear           | 774   
5 | fc.1           | Softmax          | 0     
6 | loss_fn        | CrossEntropyLoss | 0     
----------------------------------------------------
47.0 K    Trainable params
0         Non-trainable params
47.0 K    Total params
0.188     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (19) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 298.281 seconds
Epoch 133/299 ━━━━━━━━━━━━━━ 19/19 0:00:01 •       345.38it/s v_num: 7eea    
                                   0:00:00                    val_loss: 1.241
                                                              train_loss:    
                                                              1.054          
 ************ Training mlp1x64 on WISDM ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name           | Type             | Params
----------------------------------------------------
0 | backbone       | Sequential       | 46.2 K
1 | backbone.fc1   | Linear           | 46.2 K
2 | backbone.relu1 | ReLU             | 0     
3 | fc             | Sequential       | 774   
4 | fc.0           | Linear           | 774   
5 | fc.1           | Softmax          | 0     
6 | loss_fn        | CrossEntropyLoss | 0     
----------------------------------------------------
47.0 K    Trainable params
0         Non-trainable params
47.0 K    Total params
0.188     Total estimated model params size (MB)
Epoch 0/299                  0/86 0:00:00 •        0.00it/s v_num: 68c3      
                                  -:--:--                   val_loss: 1.807  
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 579, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 986, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1032, in _run_stage
    self.fit_loop.run()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 138, in run
    self.advance(data_fetcher)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 242, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 191, in run
    self._optimizer_step(batch_idx, closure)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 269, in _optimizer_step
    call._call_lightning_module_hook(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 157, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/module.py", line 1303, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/optimizer.py", line 152, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 33, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py", line 123, in step
    loss = closure()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 108, in _wrap_closure
    closure_result = closure()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 144, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 129, in closure
    step_output = self._step_fn()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 391, in training_step
    return self.lightning_module.training_step(*args, **kwargs)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 69, in training_step
    return self.single_step(batch, batch_idx, "train")
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 57, in single_step
    self.log(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/module.py", line 516, in log
    results.log(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 406, in log
    self.register_key(key, meta, value)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 423, in register_key
    metric = _ResultMetric(meta, isinstance(value, Tensor)).to(value.device)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 311, in to
    self.__dict__.update(apply_to_collection(d, (Tensor, Metric), move_data_to_device, *args, **kwargs))
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 72, in apply_to_collection
    return _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 104, in _apply_to_collection_slow
    v = _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 104, in _apply_to_collection_slow
    v = _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 96, in _apply_to_collection_slow
    return function(data, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/apply_func.py", line 102, in move_data_to_device
    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 64, in apply_to_collection
    return function(data, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/apply_func.py", line 96, in batch_to
    data_output = data.to(device, **kwargs)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/har_classification/mlp.py", line 132, in <module>
    auto_main(options)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/cli.py", line 52, in auto_main
    return pipeline.run()
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/mlflow_train.py", line 180, in run
    trainer.fit(model, datamodule)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 543, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 68, in _call_and_handle_interrupt
    trainer._teardown()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1009, in _teardown
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [11,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [12,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [28,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [29,0,0] Assertion `t >= 0 && t < n_classes` failed.
    self.strategy.teardown()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 537, in teardown
    self.lightning_module.cpu()
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/device_dtype_mixin.py", line 82, in cpu
    return super().cpu()
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 954, in cpu
    return self._apply(lambda t: t.cpu())
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 820, in _apply
    param_applied = fn(param)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 954, in <lambda>
    return self._apply(lambda t: t.cpu())
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception in thread Thread-2 (_pin_memory_loop):
Traceback (most recent call last):
  File "/usr/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py", line 54, in _pin_memory_loop
    do_one_step()
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py", line 31, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/usr/lib/python3.10/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/reductions.py", line 316, in rebuild_storage_fd
    fd = df.detach()
  File "/usr/lib/python3.10/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/usr/lib/python3.10/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 508, in Client
    answer_challenge(c, authkey)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 757, in answer_challenge
    response = connection.recv_bytes(256)        # reject large message
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer
 ************ Training tfc_transformer on KuHar ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name                                | Type                    | Params
---------------------------------------------------------------------------------
0  | time_encoder                        | TransformerEncoder      | 3.5 K 
1  | time_encoder.layers                 | ModuleList              | 3.5 K 
2  | time_encoder.layers.0               | TransformerEncoderLayer | 1.8 K 
3  | time_encoder.layers.1               | TransformerEncoderLayer | 1.8 K 
4  | time_projector                      | TFCProjectionHead       | 125 K 
5  | time_projector.layers               | Sequential              | 125 K 
6  | time_projector.layers.0             | Linear                  | 92.2 K
7  | time_projector.layers.1             | BatchNorm1d             | 512   
8  | time_projector.layers.2             | ReLU                    | 0     
9  | time_projector.layers.3             | Linear                  | 32.9 K
10 | frequency_encoder                   | TransformerEncoder      | 3.5 K 
11 | frequency_encoder.layers            | ModuleList              | 3.5 K 
12 | frequency_encoder.layers.0          | TransformerEncoderLayer | 1.8 K 
13 | frequency_encoder.layers.1          | TransformerEncoderLayer | 1.8 K 
14 | frequency_projector                 | TFCProjectionHead       | 125 K 
15 | frequency_projector.layers          | Sequential              | 125 K 
16 | frequency_projector.layers.0        | Linear                  | 92.2 K
17 | frequency_projector.layers.1        | BatchNorm1d             | 512   
18 | frequency_projector.layers.2        | ReLU                    | 0     
19 | frequency_projector.layers.3        | Linear                  | 32.9 K
20 | nxtent_criterion                    | NTXentLoss_poly         | 0     
21 | nxtent_criterion.softmax            | Softmax                 | 0     
22 | nxtent_criterion._cosine_similarity | CosineSimilarity        | 0     
23 | nxtent_criterion.criterion          | CrossEntropyLoss        | 0     
---------------------------------------------------------------------------------
258 K     Trainable params
0         Non-trainable params
258 K     Total params
1.033     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (22) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 176.953 seconds
Epoch 50/299 ━━━━━━━━━━━━━━━ 22/22 0:00:01 •       28.78it/s v_num: f537     
                                   0:00:00                   val_loss: 7.465 
                                                             train_loss:     
                                                             5.844           
 ************ Training tfc_transformer on MotionSense ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name                                | Type                    | Params
---------------------------------------------------------------------------------
0  | time_encoder                        | TransformerEncoder      | 3.5 K 
1  | time_encoder.layers                 | ModuleList              | 3.5 K 
2  | time_encoder.layers.0               | TransformerEncoderLayer | 1.8 K 
3  | time_encoder.layers.1               | TransformerEncoderLayer | 1.8 K 
4  | time_projector                      | TFCProjectionHead       | 125 K 
5  | time_projector.layers               | Sequential              | 125 K 
6  | time_projector.layers.0             | Linear                  | 92.2 K
7  | time_projector.layers.1             | BatchNorm1d             | 512   
8  | time_projector.layers.2             | ReLU                    | 0     
9  | time_projector.layers.3             | Linear                  | 32.9 K
10 | frequency_encoder                   | TransformerEncoder      | 3.5 K 
11 | frequency_encoder.layers            | ModuleList              | 3.5 K 
12 | frequency_encoder.layers.0          | TransformerEncoderLayer | 1.8 K 
13 | frequency_encoder.layers.1          | TransformerEncoderLayer | 1.8 K 
14 | frequency_projector                 | TFCProjectionHead       | 125 K 
15 | frequency_projector.layers          | Sequential              | 125 K 
16 | frequency_projector.layers.0        | Linear                  | 92.2 K
17 | frequency_projector.layers.1        | BatchNorm1d             | 512   
18 | frequency_projector.layers.2        | ReLU                    | 0     
19 | frequency_projector.layers.3        | Linear                  | 32.9 K
20 | nxtent_criterion                    | NTXentLoss_poly         | 0     
21 | nxtent_criterion.softmax            | Softmax                 | 0     
22 | nxtent_criterion._cosine_similarity | CosineSimilarity        | 0     
23 | nxtent_criterion.criterion          | CrossEntropyLoss        | 0     
---------------------------------------------------------------------------------
258 K     Trainable params
0         Non-trainable params
258 K     Total params
1.033     Total estimated model params size (MB)
--> Overall fit time: 518.228 seconds
Epoch 104/299 ━━━━━━━━━━━━━━━ 56/56 0:00:03 •       29.91it/s v_num: 9352    
                                    0:00:00                   val_loss: 7.033
                                                              train_loss:    
                                                              5.791          
 ************ Training tfc_transformer on RealWorld_thigh ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name                                | Type                    | Params
---------------------------------------------------------------------------------
0  | time_encoder                        | TransformerEncoder      | 3.5 K 
1  | time_encoder.layers                 | ModuleList              | 3.5 K 
2  | time_encoder.layers.0               | TransformerEncoderLayer | 1.8 K 
3  | time_encoder.layers.1               | TransformerEncoderLayer | 1.8 K 
4  | time_projector                      | TFCProjectionHead       | 125 K 
5  | time_projector.layers               | Sequential              | 125 K 
6  | time_projector.layers.0             | Linear                  | 92.2 K
7  | time_projector.layers.1             | BatchNorm1d             | 512   
8  | time_projector.layers.2             | ReLU                    | 0     
9  | time_projector.layers.3             | Linear                  | 32.9 K
10 | frequency_encoder                   | TransformerEncoder      | 3.5 K 
11 | frequency_encoder.layers            | ModuleList              | 3.5 K 
12 | frequency_encoder.layers.0          | TransformerEncoderLayer | 1.8 K 
13 | frequency_encoder.layers.1          | TransformerEncoderLayer | 1.8 K 
14 | frequency_projector                 | TFCProjectionHead       | 125 K 
15 | frequency_projector.layers          | Sequential              | 125 K 
16 | frequency_projector.layers.0        | Linear                  | 92.2 K
17 | frequency_projector.layers.1        | BatchNorm1d             | 512   
18 | frequency_projector.layers.2        | ReLU                    | 0     
19 | frequency_projector.layers.3        | Linear                  | 32.9 K
20 | nxtent_criterion                    | NTXentLoss_poly         | 0     
21 | nxtent_criterion.softmax            | Softmax                 | 0     
22 | nxtent_criterion._cosine_similarity | CosineSimilarity        | 0     
23 | nxtent_criterion.criterion          | CrossEntropyLoss        | 0     
---------------------------------------------------------------------------------
258 K     Trainable params
0         Non-trainable params
258 K     Total params
1.033     Total estimated model params size (MB)
--> Overall fit time: 1555.071 seconds
Epoch 196/299 ━━━━━━━━━━━━━━ 132/132 0:00:06 •      27.13it/s v_num: dd9c    
                                     0:00:00                  val_loss: 7.369
                                                              train_loss:    
                                                              5.768          
 ************ Training tfc_transformer on RealWorld_waist ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name                                | Type                    | Params
---------------------------------------------------------------------------------
0  | time_encoder                        | TransformerEncoder      | 3.5 K 
1  | time_encoder.layers                 | ModuleList              | 3.5 K 
2  | time_encoder.layers.0               | TransformerEncoderLayer | 1.8 K 
3  | time_encoder.layers.1               | TransformerEncoderLayer | 1.8 K 
4  | time_projector                      | TFCProjectionHead       | 125 K 
5  | time_projector.layers               | Sequential              | 125 K 
6  | time_projector.layers.0             | Linear                  | 92.2 K
7  | time_projector.layers.1             | BatchNorm1d             | 512   
8  | time_projector.layers.2             | ReLU                    | 0     
9  | time_projector.layers.3             | Linear                  | 32.9 K
10 | frequency_encoder                   | TransformerEncoder      | 3.5 K 
11 | frequency_encoder.layers            | ModuleList              | 3.5 K 
12 | frequency_encoder.layers.0          | TransformerEncoderLayer | 1.8 K 
13 | frequency_encoder.layers.1          | TransformerEncoderLayer | 1.8 K 
14 | frequency_projector                 | TFCProjectionHead       | 125 K 
15 | frequency_projector.layers          | Sequential              | 125 K 
16 | frequency_projector.layers.0        | Linear                  | 92.2 K
17 | frequency_projector.layers.1        | BatchNorm1d             | 512   
18 | frequency_projector.layers.2        | ReLU                    | 0     
19 | frequency_projector.layers.3        | Linear                  | 32.9 K
20 | nxtent_criterion                    | NTXentLoss_poly         | 0     
21 | nxtent_criterion.softmax            | Softmax                 | 0     
22 | nxtent_criterion._cosine_similarity | CosineSimilarity        | 0     
23 | nxtent_criterion.criterion          | CrossEntropyLoss        | 0     
---------------------------------------------------------------------------------
258 K     Trainable params
0         Non-trainable params
258 K     Total params
1.033     Total estimated model params size (MB)
--> Overall fit time: 1877.615 seconds
Epoch 204/299 ━━━━━━━━━━━━━━━━━━━━ 162/162 0:00:07 • 0:00:00 26.26it/s v_num: 3eee         
                                                                       val_loss: 6.798     
                                                                       train_loss: 5.763   
 ************ Training tfc_transformer on UCI ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name                                | Type                    | Params
---------------------------------------------------------------------------------
0  | time_encoder                        | TransformerEncoder      | 3.5 K 
1  | time_encoder.layers                 | ModuleList              | 3.5 K 
2  | time_encoder.layers.0               | TransformerEncoderLayer | 1.8 K 
3  | time_encoder.layers.1               | TransformerEncoderLayer | 1.8 K 
4  | time_projector                      | TFCProjectionHead       | 125 K 
5  | time_projector.layers               | Sequential              | 125 K 
6  | time_projector.layers.0             | Linear                  | 92.2 K
7  | time_projector.layers.1             | BatchNorm1d             | 512   
8  | time_projector.layers.2             | ReLU                    | 0     
9  | time_projector.layers.3             | Linear                  | 32.9 K
10 | frequency_encoder                   | TransformerEncoder      | 3.5 K 
11 | frequency_encoder.layers            | ModuleList              | 3.5 K 
12 | frequency_encoder.layers.0          | TransformerEncoderLayer | 1.8 K 
13 | frequency_encoder.layers.1          | TransformerEncoderLayer | 1.8 K 
14 | frequency_projector                 | TFCProjectionHead       | 125 K 
15 | frequency_projector.layers          | Sequential              | 125 K 
16 | frequency_projector.layers.0        | Linear                  | 92.2 K
17 | frequency_projector.layers.1        | BatchNorm1d             | 512   
18 | frequency_projector.layers.2        | ReLU                    | 0     
19 | frequency_projector.layers.3        | Linear                  | 32.9 K
20 | nxtent_criterion                    | NTXentLoss_poly         | 0     
21 | nxtent_criterion.softmax            | Softmax                 | 0     
22 | nxtent_criterion._cosine_similarity | CosineSimilarity        | 0     
23 | nxtent_criterion.criterion          | CrossEntropyLoss        | 0     
---------------------------------------------------------------------------------
258 K     Trainable params
0         Non-trainable params
258 K     Total params
1.033     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (38) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 562.110 seconds
Epoch 134/299 ━━━━━━━━━━━━━━━━━━━━ 38/38 0:00:02 • 0:00:00 28.13it/s v_num: 1978 val_loss: 
                                                                     6.815 train_loss:     
                                                                     5.885                 
 ************ Training tfc_transformer on WISDM ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name                                | Type                    | Params
---------------------------------------------------------------------------------
0  | time_encoder                        | TransformerEncoder      | 3.5 K 
1  | time_encoder.layers                 | ModuleList              | 3.5 K 
2  | time_encoder.layers.0               | TransformerEncoderLayer | 1.8 K 
3  | time_encoder.layers.1               | TransformerEncoderLayer | 1.8 K 
4  | time_projector                      | TFCProjectionHead       | 125 K 
5  | time_projector.layers               | Sequential              | 125 K 
6  | time_projector.layers.0             | Linear                  | 92.2 K
7  | time_projector.layers.1             | BatchNorm1d             | 512   
8  | time_projector.layers.2             | ReLU                    | 0     
9  | time_projector.layers.3             | Linear                  | 32.9 K
10 | frequency_encoder                   | TransformerEncoder      | 3.5 K 
11 | frequency_encoder.layers            | ModuleList              | 3.5 K 
12 | frequency_encoder.layers.0          | TransformerEncoderLayer | 1.8 K 
13 | frequency_encoder.layers.1          | TransformerEncoderLayer | 1.8 K 
14 | frequency_projector                 | TFCProjectionHead       | 125 K 
15 | frequency_projector.layers          | Sequential              | 125 K 
16 | frequency_projector.layers.0        | Linear                  | 92.2 K
17 | frequency_projector.layers.1        | BatchNorm1d             | 512   
18 | frequency_projector.layers.2        | ReLU                    | 0     
19 | frequency_projector.layers.3        | Linear                  | 32.9 K
20 | nxtent_criterion                    | NTXentLoss_poly         | 0     
21 | nxtent_criterion.softmax            | Softmax                 | 0     
22 | nxtent_criterion._cosine_similarity | CosineSimilarity        | 0     
23 | nxtent_criterion.criterion          | CrossEntropyLoss        | 0     
---------------------------------------------------------------------------------
258 K     Trainable params
0         Non-trainable params
258 K     Total params
1.033     Total estimated model params size (MB)
--> Overall fit time: 1913.223 seconds
Epoch 185/299 ━━━━━━━━━━━━━━━━━━━━ 171/171 0:00:08 • 0:00:00 24.69it/s v_num: 988f         
                                                                       val_loss: 6.666     
                                                                       train_loss: 5.807   
 ************ Training gru1l128 on KuHar ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name         | Type             | Params
--------------------------------------------------
0 | backbone     | GRUEncoder       | 90.5 K
1 | backbone.rnn | GRU              | 64.8 K
2 | backbone.nn  | Linear           | 25.7 K
3 | fc           | Linear           | 774   
4 | loss_fn      | CrossEntropyLoss | 0     
--------------------------------------------------
91.3 K    Trainable params
0         Non-trainable params
91.3 K    Total params
0.365     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 80.375 seconds
Epoch 33/299 ━━━━━━━━━━━━━━━━━━━━ 11/11 0:00:01 • 0:00:00 317.89it/s v_num: db63 val_loss: 
                                                                     2.858 train_loss:     
                                                                     0.247                 
 ************ Training gru1l128 on MotionSense ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name         | Type             | Params
--------------------------------------------------
0 | backbone     | GRUEncoder       | 90.5 K
1 | backbone.rnn | GRU              | 64.8 K
2 | backbone.nn  | Linear           | 25.7 K
3 | fc           | Linear           | 774   
4 | loss_fn      | CrossEntropyLoss | 0     
--------------------------------------------------
91.3 K    Trainable params
0         Non-trainable params
91.3 K    Total params
0.365     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (28) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 99.690 seconds
Epoch 39/299 ━━━━━━━━━━━━━━━━━━━━ 28/28 0:00:01 • 0:00:00 297.90it/s v_num: cb84 val_loss: 
                                                                     1.676 train_loss:     
                                                                     0.240                 
 ************ Training gru1l128 on RealWorld_thigh ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name         | Type             | Params
--------------------------------------------------
0 | backbone     | GRUEncoder       | 90.5 K
1 | backbone.rnn | GRU              | 64.8 K
2 | backbone.nn  | Linear           | 25.7 K
3 | fc           | Linear           | 774   
4 | loss_fn      | CrossEntropyLoss | 0     
--------------------------------------------------
91.3 K    Trainable params
0         Non-trainable params
91.3 K    Total params
0.365     Total estimated model params size (MB)
--> Overall fit time: 82.816 seconds
Epoch 30/299 ━━━━━━━━━━━━━━━━━━━━ 66/66 0:00:01 • 0:00:00 223.36it/s v_num: 429c val_loss: 
                                                                     3.436 train_loss:     
                                                                     0.213                 
 ************ Training gru1l128 on RealWorld_waist ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name         | Type             | Params
--------------------------------------------------
0 | backbone     | GRUEncoder       | 90.5 K
1 | backbone.rnn | GRU              | 64.8 K
2 | backbone.nn  | Linear           | 25.7 K
3 | fc           | Linear           | 774   
4 | loss_fn      | CrossEntropyLoss | 0     
--------------------------------------------------
91.3 K    Trainable params
0         Non-trainable params
91.3 K    Total params
0.365     Total estimated model params size (MB)
--> Overall fit time: 102.264 seconds
Epoch 36/299 ━━━━━━━━━━━━━━━━━━━━ 81/81 0:00:01 • 0:00:00 221.01it/s v_num: 2469 val_loss: 
                                                                     2.025 train_loss:     
                                                                     0.251                 
 ************ Training gru1l128 on UCI ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name         | Type             | Params
--------------------------------------------------
0 | backbone     | GRUEncoder       | 90.5 K
1 | backbone.rnn | GRU              | 64.8 K
2 | backbone.nn  | Linear           | 25.7 K
3 | fc           | Linear           | 774   
4 | loss_fn      | CrossEntropyLoss | 0     
--------------------------------------------------
91.3 K    Trainable params
0         Non-trainable params
91.3 K    Total params
0.365     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (19) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 257.784 seconds
Epoch 106/299 ━━━━━━━━━━━━━━━━━━━━ 19/19 0:00:01 • 0:00:00 324.32it/s v_num: 7904 val_loss:
                                                                      0.281 train_loss:    
                                                                      0.240                
 ************ Training gru1l128 on WISDM ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name         | Type             | Params
--------------------------------------------------
0 | backbone     | GRUEncoder       | 90.5 K
1 | backbone.rnn | GRU              | 64.8 K
2 | backbone.nn  | Linear           | 25.7 K
3 | fc           | Linear           | 774   
4 | loss_fn      | CrossEntropyLoss | 0     
--------------------------------------------------
91.3 K    Trainable params
0         Non-trainable params
91.3 K    Total params
0.365     Total estimated model params size (MB)
Epoch 0/299                        0/86 0:00:00 • -:--:-- 0.00it/s v_num: 6f4a val_loss:   
                                                                   1.773                   
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [11,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [14,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [15,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [28,0,0] Assertion `t >= 0 && t < n_classes` failed.
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 579, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 986, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1032, in _run_stage
    self.fit_loop.run()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 138, in run
    self.advance(data_fetcher)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 242, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 191, in run
    self._optimizer_step(batch_idx, closure)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 269, in _optimizer_step
    call._call_lightning_module_hook(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 157, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/module.py", line 1303, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/optimizer.py", line 152, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 33, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py", line 123, in step
    loss = closure()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 108, in _wrap_closure
    closure_result = closure()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 144, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 129, in closure
    step_output = self._step_fn()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 391, in training_step
    return self.lightning_module.training_step(*args, **kwargs)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 69, in training_step
    return self.single_step(batch, batch_idx, "train")
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 57, in single_step
    self.log(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/module.py", line 516, in log
    results.log(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 406, in log
    self.register_key(key, meta, value)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 423, in register_key
    metric = _ResultMetric(meta, isinstance(value, Tensor)).to(value.device)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 311, in to
    self.__dict__.update(apply_to_collection(d, (Tensor, Metric), move_data_to_device, *args, **kwargs))
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 72, in apply_to_collection
    return _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 104, in _apply_to_collection_slow
    v = _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 104, in _apply_to_collection_slow
    v = _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 96, in _apply_to_collection_slow
    return function(data, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/apply_func.py", line 102, in move_data_to_device
    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 64, in apply_to_collection
    return function(data, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/apply_func.py", line 96, in batch_to
    data_output = data.to(device, **kwargs)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/har_classification/gru_encoder.py", line 168, in <module>
    auto_main(options)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/cli.py", line 52, in auto_main
    return pipeline.run()
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/mlflow_train.py", line 180, in run
    trainer.fit(model, datamodule)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 543, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 68, in _call_and_handle_interrupt
    trainer._teardown()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1009, in _teardown
    self.strategy.teardown()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 537, in teardown
    self.lightning_module.cpu()
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/device_dtype_mixin.py", line 82, in cpu
    return super().cpu()
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 954, in cpu
    return self._apply(lambda t: t.cpu())
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py", line 197, in _apply
    ret = super()._apply(fn)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 820, in _apply
    param_applied = fn(param)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 954, in <lambda>
    return self._apply(lambda t: t.cpu())
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception in thread Thread-2 (_pin_memory_loop):
Traceback (most recent call last):
  File "/usr/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py", line 54, in _pin_memory_loop
    do_one_step()
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py", line 31, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/usr/lib/python3.10/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/reductions.py", line 316, in rebuild_storage_fd
    fd = df.detach()
  File "/usr/lib/python3.10/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/usr/lib/python3.10/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 508, in Client
    answer_challenge(c, authkey)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 752, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer
 ************ Training simple2Dconv on KuHar ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name       | Type             | Params
-------------------------------------------------
0  | backbone   | Sequential       | 13.5 K
1  | backbone.0 | Conv2d           | 1.2 K 
2  | backbone.1 | ReLU             | 0     
3  | backbone.2 | MaxPool2d        | 0     
4  | backbone.3 | Conv2d           | 12.4 K
5  | backbone.4 | ReLU             | 0     
6  | backbone.5 | MaxPool2d        | 0     
7  | fc         | Sequential       | 1.2 M 
8  | fc.0       | Linear           | 705 K 
9  | fc.1       | ReLU             | 0     
10 | fc.2       | Linear           | 500 K 
11 | fc.3       | ReLU             | 0     
12 | fc.4       | Linear           | 3.0 K 
13 | loss_fn    | CrossEntropyLoss | 0     
-------------------------------------------------
1.2 M     Trainable params
0         Non-trainable params
1.2 M     Total params
4.888     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 79.710 seconds
Epoch 30/299 ━━━━━━━━━━━━━━━━━━━━ 11/11 0:00:01 • 0:00:00 330.73it/s v_num: a25b val_loss: 
                                                                     7.843 train_loss:     
                                                                     0.236                 
 ************ Training simple2Dconv on MotionSense ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name       | Type             | Params
-------------------------------------------------
0  | backbone   | Sequential       | 13.5 K
1  | backbone.0 | Conv2d           | 1.2 K 
2  | backbone.1 | ReLU             | 0     
3  | backbone.2 | MaxPool2d        | 0     
4  | backbone.3 | Conv2d           | 12.4 K
5  | backbone.4 | ReLU             | 0     
6  | backbone.5 | MaxPool2d        | 0     
7  | fc         | Sequential       | 1.2 M 
8  | fc.0       | Linear           | 705 K 
9  | fc.1       | ReLU             | 0     
10 | fc.2       | Linear           | 500 K 
11 | fc.3       | ReLU             | 0     
12 | fc.4       | Linear           | 3.0 K 
13 | loss_fn    | CrossEntropyLoss | 0     
-------------------------------------------------
1.2 M     Trainable params
0         Non-trainable params
1.2 M     Total params
4.888     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (28) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 153.870 seconds
Epoch 55/299 ━━━━━━━━━━━━━━━━━━━━ 28/28 0:00:01 • 0:00:00 209.08it/s v_num: 9afe val_loss: 
                                                                     3.196 train_loss:     
                                                                     0.001                 
 ************ Training simple2Dconv on RealWorld_thigh ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name       | Type             | Params
-------------------------------------------------
0  | backbone   | Sequential       | 13.5 K
1  | backbone.0 | Conv2d           | 1.2 K 
2  | backbone.1 | ReLU             | 0     
3  | backbone.2 | MaxPool2d        | 0     
4  | backbone.3 | Conv2d           | 12.4 K
5  | backbone.4 | ReLU             | 0     
6  | backbone.5 | MaxPool2d        | 0     
7  | fc         | Sequential       | 1.2 M 
8  | fc.0       | Linear           | 705 K 
9  | fc.1       | ReLU             | 0     
10 | fc.2       | Linear           | 500 K 
11 | fc.3       | ReLU             | 0     
12 | fc.4       | Linear           | 3.0 K 
13 | loss_fn    | CrossEntropyLoss | 0     
-------------------------------------------------
1.2 M     Trainable params
0         Non-trainable params
1.2 M     Total params
4.888     Total estimated model params size (MB)
--> Overall fit time: 87.212 seconds
Epoch 30/299 ━━━━━━━━━━━━━━━━━━━━━━ 66/66 0:00:01 • 0:00:00 209.70it/s v_num: 6e05 val_loss:  
                                                                       2.527 train_loss: 0.004
 ************ Training simple2Dconv on RealWorld_waist ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name       | Type             | Params
-------------------------------------------------
0  | backbone   | Sequential       | 13.5 K
1  | backbone.0 | Conv2d           | 1.2 K 
2  | backbone.1 | ReLU             | 0     
3  | backbone.2 | MaxPool2d        | 0     
4  | backbone.3 | Conv2d           | 12.4 K
5  | backbone.4 | ReLU             | 0     
6  | backbone.5 | MaxPool2d        | 0     
7  | fc         | Sequential       | 1.2 M 
8  | fc.0       | Linear           | 705 K 
9  | fc.1       | ReLU             | 0     
10 | fc.2       | Linear           | 500 K 
11 | fc.3       | ReLU             | 0     
12 | fc.4       | Linear           | 3.0 K 
13 | loss_fn    | CrossEntropyLoss | 0     
-------------------------------------------------
1.2 M     Trainable params
0         Non-trainable params
1.2 M     Total params
4.888     Total estimated model params size (MB)
--> Overall fit time: 106.625 seconds
Epoch 33/299 ━━━━━━━━━━━━━━━━━━━━━━ 81/81 0:00:01 • 0:00:00 277.13it/s v_num: 7757 val_loss:  
                                                                       2.105 train_loss: 0.008
 ************ Training simple2Dconv on UCI ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name       | Type             | Params
-------------------------------------------------
0  | backbone   | Sequential       | 13.5 K
1  | backbone.0 | Conv2d           | 1.2 K 
2  | backbone.1 | ReLU             | 0     
3  | backbone.2 | MaxPool2d        | 0     
4  | backbone.3 | Conv2d           | 12.4 K
5  | backbone.4 | ReLU             | 0     
6  | backbone.5 | MaxPool2d        | 0     
7  | fc         | Sequential       | 1.2 M 
8  | fc.0       | Linear           | 705 K 
9  | fc.1       | ReLU             | 0     
10 | fc.2       | Linear           | 500 K 
11 | fc.3       | ReLU             | 0     
12 | fc.4       | Linear           | 3.0 K 
13 | loss_fn    | CrossEntropyLoss | 0     
-------------------------------------------------
1.2 M     Trainable params
0         Non-trainable params
1.2 M     Total params
4.888     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (19) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 142.600 seconds
Epoch 55/299 ━━━━━━━━━━━━━━━━━━━━━━ 19/19 0:00:01 • 0:00:00 268.97it/s v_num: 34a5 val_loss:  
                                                                       0.354 train_loss: 0.015
 ************ Training simple2Dconv on WISDM ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

   | Name       | Type             | Params
-------------------------------------------------
0  | backbone   | Sequential       | 13.5 K
1  | backbone.0 | Conv2d           | 1.2 K 
2  | backbone.1 | ReLU             | 0     
3  | backbone.2 | MaxPool2d        | 0     
4  | backbone.3 | Conv2d           | 12.4 K
5  | backbone.4 | ReLU             | 0     
6  | backbone.5 | MaxPool2d        | 0     
7  | fc         | Sequential       | 1.2 M 
8  | fc.0       | Linear           | 705 K 
9  | fc.1       | ReLU             | 0     
10 | fc.2       | Linear           | 500 K 
11 | fc.3       | ReLU             | 0     
12 | fc.4       | Linear           | 3.0 K 
13 | loss_fn    | CrossEntropyLoss | 0     
-------------------------------------------------
1.2 M     Trainable params
0         Non-trainable params
1.2 M     Total params
4.888     Total estimated model params size (MB)
Epoch 0/299                          0/86 0:00:00 • -:--:-- 0.00it/s v_num: 88c5 val_loss:    
                                                                     1.758                    
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [10,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [11,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [16,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [17,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [21,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [24,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [28,0,0] Assertion `t >= 0 && t < n_classes` failed.
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 579, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 986, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1032, in _run_stage
    self.fit_loop.run()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 138, in run
    self.advance(data_fetcher)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 242, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 191, in run
    self._optimizer_step(batch_idx, closure)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 269, in _optimizer_step
    call._call_lightning_module_hook(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 157, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/module.py", line 1303, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/optimizer.py", line 152, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 33, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py", line 123, in step
    loss = closure()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 108, in _wrap_closure
    closure_result = closure()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 144, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 129, in closure
    step_output = self._step_fn()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 391, in training_step
    return self.lightning_module.training_step(*args, **kwargs)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 69, in training_step
    return self.single_step(batch, batch_idx, "train")
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 57, in single_step
    self.log(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/module.py", line 516, in log
    results.log(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 406, in log
    self.register_key(key, meta, value)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 423, in register_key
    metric = _ResultMetric(meta, isinstance(value, Tensor)).to(value.device)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 311, in to
    self.__dict__.update(apply_to_collection(d, (Tensor, Metric), move_data_to_device, *args, **kwargs))
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 72, in apply_to_collection
    return _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 104, in _apply_to_collection_slow
    v = _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 104, in _apply_to_collection_slow
    v = _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 96, in _apply_to_collection_slow
    return function(data, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/apply_func.py", line 102, in move_data_to_device
    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 64, in apply_to_collection
    return function(data, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/apply_func.py", line 96, in batch_to
    data_output = data.to(device, **kwargs)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/har_classification/simple2Dconv_classifier.py", line 135, in <module>
    auto_main(options)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/cli.py", line 52, in auto_main
    return pipeline.run()
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/mlflow_train.py", line 180, in run
    trainer.fit(model, datamodule)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 543, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 68, in _call_and_handle_interrupt
    trainer._teardown()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1009, in _teardown
    self.strategy.teardown()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 537, in teardown
    self.lightning_module.cpu()
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/device_dtype_mixin.py", line 82, in cpu
    return super().cpu()
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 954, in cpu
    return self._apply(lambda t: t.cpu())
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 820, in _apply
    param_applied = fn(param)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 954, in <lambda>
    return self._apply(lambda t: t.cpu())
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception in thread Thread-2 (_pin_memory_loop):
Traceback (most recent call last):
  File "/usr/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py", line 54, in _pin_memory_loop
    do_one_step()
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py", line 31, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/usr/lib/python3.10/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/reductions.py", line 316, in rebuild_storage_fd
    fd = df.detach()
  File "/usr/lib/python3.10/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/usr/lib/python3.10/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 509, in Client
    deliver_challenge(c, authkey)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 740, in deliver_challenge
    response = connection.recv_bytes(256)        # reject large message
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer
 ************ Training mlp2x64_fft on KuHar ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name           | Type             | Params
----------------------------------------------------
0 | backbone       | Sequential       | 39.7 K
1 | backbone.fc1   | Linear           | 23.2 K
2 | backbone.relu1 | ReLU             | 0     
3 | backbone.fc2   | Linear           | 16.5 K
4 | backbone.relu2 | ReLU             | 0     
5 | fc             | Sequential       | 774   
6 | fc.0           | Linear           | 774   
7 | fc.1           | Softmax          | 0     
8 | loss_fn        | CrossEntropyLoss | 0     
----------------------------------------------------
40.5 K    Trainable params
0         Non-trainable params
40.5 K    Total params
0.162     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 117.697 seconds
Epoch 52/299 ━━━━━━━━━━━━━━━━━━━━━━ 11/11 0:00:01 • 0:00:00 400.47it/s v_num: 8593 val_loss:  
                                                                       1.378 train_loss: 1.053
 ************ Training mlp2x64_fft on MotionSense ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name           | Type             | Params
----------------------------------------------------
0 | backbone       | Sequential       | 39.7 K
1 | backbone.fc1   | Linear           | 23.2 K
2 | backbone.relu1 | ReLU             | 0     
3 | backbone.fc2   | Linear           | 16.5 K
4 | backbone.relu2 | ReLU             | 0     
5 | fc             | Sequential       | 774   
6 | fc.0           | Linear           | 774   
7 | fc.1           | Softmax          | 0     
8 | loss_fn        | CrossEntropyLoss | 0     
----------------------------------------------------
40.5 K    Trainable params
0         Non-trainable params
40.5 K    Total params
0.162     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (28) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 204.838 seconds
Epoch 87/299 ━━━━━━━━━━━━━━━━━━━━━━ 28/28 0:00:01 • 0:00:00 347.64it/s v_num: d14d val_loss:  
                                                                       1.295 train_loss: 1.061
 ************ Training mlp2x64_fft on RealWorld_thigh ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name           | Type             | Params
----------------------------------------------------
0 | backbone       | Sequential       | 39.7 K
1 | backbone.fc1   | Linear           | 23.2 K
2 | backbone.relu1 | ReLU             | 0     
3 | backbone.fc2   | Linear           | 16.5 K
4 | backbone.relu2 | ReLU             | 0     
5 | fc             | Sequential       | 774   
6 | fc.0           | Linear           | 774   
7 | fc.1           | Softmax          | 0     
8 | loss_fn        | CrossEntropyLoss | 0     
----------------------------------------------------
40.5 K    Trainable params
0         Non-trainable params
40.5 K    Total params
0.162     Total estimated model params size (MB)
--> Overall fit time: 92.123 seconds
Epoch 36/299 ━━━━━━━━━━━━━━━━━━━━━━ 66/66 0:00:01 • 0:00:00 367.99it/s v_num: 1aa6 val_loss:  
                                                                       1.383 train_loss: 1.239
 ************ Training mlp2x64_fft on RealWorld_waist ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name           | Type             | Params
----------------------------------------------------
0 | backbone       | Sequential       | 39.7 K
1 | backbone.fc1   | Linear           | 23.2 K
2 | backbone.relu1 | ReLU             | 0     
3 | backbone.fc2   | Linear           | 16.5 K
4 | backbone.relu2 | ReLU             | 0     
5 | fc             | Sequential       | 774   
6 | fc.0           | Linear           | 774   
7 | fc.1           | Softmax          | 0     
8 | loss_fn        | CrossEntropyLoss | 0     
----------------------------------------------------
40.5 K    Trainable params
0         Non-trainable params
40.5 K    Total params
0.162     Total estimated model params size (MB)
--> Overall fit time: 132.686 seconds
Epoch 52/299 ━━━━━━━━━━━━━━━━━━━━━━ 81/81 0:00:01 • 0:00:00 321.21it/s v_num: 291d val_loss:  
                                                                       1.279 train_loss: 1.096
 ************ Training mlp2x64_fft on UCI ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name           | Type             | Params
----------------------------------------------------
0 | backbone       | Sequential       | 39.7 K
1 | backbone.fc1   | Linear           | 23.2 K
2 | backbone.relu1 | ReLU             | 0     
3 | backbone.fc2   | Linear           | 16.5 K
4 | backbone.relu2 | ReLU             | 0     
5 | fc             | Sequential       | 774   
6 | fc.0           | Linear           | 774   
7 | fc.1           | Softmax          | 0     
8 | loss_fn        | CrossEntropyLoss | 0     
----------------------------------------------------
40.5 K    Trainable params
0         Non-trainable params
40.5 K    Total params
0.162     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (19) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 210.345 seconds
Epoch 90/299 ━━━━━━━━━━━━━━━━━━━━━━ 19/19 0:00:01 • 0:00:00 265.28it/s v_num: 1b08 val_loss:  
                                                                       1.187 train_loss: 1.057
 ************ Training mlp2x64_fft on WISDM ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name           | Type             | Params
----------------------------------------------------
0 | backbone       | Sequential       | 39.7 K
1 | backbone.fc1   | Linear           | 23.2 K
2 | backbone.relu1 | ReLU             | 0     
3 | backbone.fc2   | Linear           | 16.5 K
4 | backbone.relu2 | ReLU             | 0     
5 | fc             | Sequential       | 774   
6 | fc.0           | Linear           | 774   
7 | fc.1           | Softmax          | 0     
8 | loss_fn        | CrossEntropyLoss | 0     
----------------------------------------------------
40.5 K    Trainable params
0         Non-trainable params
40.5 K    Total params
0.162     Total estimated model params size (MB)
Epoch 0/299                          0/86 0:00:00 • -:--:-- 0.00it/s v_num: 670b val_loss:    
                                                                     1.442                    
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 579, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 986, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1032, in _run_stage
    self.fit_loop.run()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 138, in run
    self.advance(data_fetcher)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 242, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 191, in run
    self._optimizer_step(batch_idx, closure)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 269, in _optimizer_step
    call._call_lightning_module_hook(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 157, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/module.py", line 1303, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/optimizer.py", line 152, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 33, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py", line 123, in step
    loss = closure()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 108, in _wrap_closure
    closure_result = closure()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 144, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 129, in closure
    step_output = self._step_fn()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 391, in training_step
    return self.lightning_module.training_step(*args, **kwargs)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 69, in training_step
    return self.single_step(batch, batch_idx, "train")
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 57, in single_step
    self.log(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/module.py", line 516, in log
    results.log(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 406, in log
    self.register_key(key, meta, value)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 423, in register_key
    metric = _ResultMetric(meta, isinstance(value, Tensor)).to(value.device)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 311, in to
    self.__dict__.update(apply_to_collection(d, (Tensor, Metric), move_data_to_device, *args, **kwargs))
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 72, in apply_to_collection
    return _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 104, in _apply_to_collection_slow
    v = _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 104, in _apply_to_collection_slow
    v = _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 96, in _apply_to_collection_slow
    return function(data, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/apply_func.py", line 102, in move_data_to_device
    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 64, in apply_to_collection
    return function(data, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/apply_func.py", line 96, in batch_to
    data_output = data.to(device, **kwargs)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/har_classification/mlp.py", line 132, in <module>
    auto_main(options)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/cli.py", line 52, in auto_main
    return pipeline.run()
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/mlflow_train.py", line 180, in run
    trainer.fit(model, datamodule)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 543, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 68, in _call_and_handle_interrupt
    trainer._teardown()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1009, in _teardown
    self.strategy.teardown()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 537, in teardown
    self.lightning_module.cpu()
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/device_dtype_mixin.py", line 82, in cpu
    return super().cpu()
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 954, in cpu
    return self._apply(lambda t: t.cpu())
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 820, in _apply
    param_applied = fn(param)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 954, in <lambda>
    return self._apply(lambda t: t.cpu())
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [10,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [12,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [14,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [20,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [26,0,0] Assertion `t >= 0 && t < n_classes` failed.
Exception in thread Thread-2 (_pin_memory_loop):
Traceback (most recent call last):
  File "/usr/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py", line 54, in _pin_memory_loop
    do_one_step()
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py", line 31, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/usr/lib/python3.10/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/reductions.py", line 316, in rebuild_storage_fd
    fd = df.detach()
  File "/usr/lib/python3.10/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/usr/lib/python3.10/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 508, in Client
    answer_challenge(c, authkey)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 752, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer
 ************ Training gru2l128 on KuHar ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name         | Type             | Params
--------------------------------------------------
0 | backbone     | GRUEncoder       | 271 K 
1 | backbone.rnn | GRU              | 246 K 
2 | backbone.nn  | Linear           | 25.7 K
3 | fc           | Linear           | 774   
4 | loss_fn      | CrossEntropyLoss | 0     
--------------------------------------------------
272 K     Trainable params
0         Non-trainable params
272 K     Total params
1.090     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 75.883 seconds
Epoch 32/299 ━━━━━━━━━━━━━━━━━━━━━━ 11/11 0:00:01 • 0:00:00 269.06it/s v_num: ae38 val_loss:  
                                                                       3.420 train_loss: 0.234
 ************ Training gru2l128 on MotionSense ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name         | Type             | Params
--------------------------------------------------
0 | backbone     | GRUEncoder       | 271 K 
1 | backbone.rnn | GRU              | 246 K 
2 | backbone.nn  | Linear           | 25.7 K
3 | fc           | Linear           | 774   
4 | loss_fn      | CrossEntropyLoss | 0     
--------------------------------------------------
272 K     Trainable params
0         Non-trainable params
272 K     Total params
1.090     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (28) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 110.338 seconds
Epoch 43/299 ━━━━━━━━━━━━━━━━━━━━━━ 28/28 0:00:01 • 0:00:00 207.96it/s v_num: 8716 val_loss:  
                                                                       1.301 train_loss: 0.109
 ************ Training gru2l128 on RealWorld_thigh ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name         | Type             | Params
--------------------------------------------------
0 | backbone     | GRUEncoder       | 271 K 
1 | backbone.rnn | GRU              | 246 K 
2 | backbone.nn  | Linear           | 25.7 K
3 | fc           | Linear           | 774   
4 | loss_fn      | CrossEntropyLoss | 0     
--------------------------------------------------
272 K     Trainable params
0         Non-trainable params
272 K     Total params
1.090     Total estimated model params size (MB)
--> Overall fit time: 85.124 seconds
Epoch 30/299 ━━━━━━━━━━━━━━━━━━━━━━ 66/66 0:00:01 • 0:00:00 190.96it/s v_num: 2b6f val_loss:  
                                                                       3.342 train_loss: 0.083
 ************ Training gru2l128 on RealWorld_waist ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name         | Type             | Params
--------------------------------------------------
0 | backbone     | GRUEncoder       | 271 K 
1 | backbone.rnn | GRU              | 246 K 
2 | backbone.nn  | Linear           | 25.7 K
3 | fc           | Linear           | 774   
4 | loss_fn      | CrossEntropyLoss | 0     
--------------------------------------------------
272 K     Trainable params
0         Non-trainable params
272 K     Total params
1.090     Total estimated model params size (MB)
--> Overall fit time: 120.993 seconds
Epoch 42/299 ━━━━━━━━━━━━━━━━━━━━━━ 81/81 0:00:01 • 0:00:00 196.99it/s v_num: e0e9 val_loss:  
                                                                       1.668 train_loss: 0.061
 ************ Training gru2l128 on UCI ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name         | Type             | Params
--------------------------------------------------
0 | backbone     | GRUEncoder       | 271 K 
1 | backbone.rnn | GRU              | 246 K 
2 | backbone.nn  | Linear           | 25.7 K
3 | fc           | Linear           | 774   
4 | loss_fn      | CrossEntropyLoss | 0     
--------------------------------------------------
272 K     Trainable params
0         Non-trainable params
272 K     Total params
1.090     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (19) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 319.875 seconds
Epoch 139/299 ━━━━━━━━━━━━━━━━━━━━━━ 19/19 0:00:01 • 0:00:00 282.51it/s v_num: a26e val_loss: 
                                                                        0.308 train_loss:     
                                                                        0.013                 
 ************ Training gru2l128 on WISDM ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name         | Type             | Params
--------------------------------------------------
0 | backbone     | GRUEncoder       | 271 K 
1 | backbone.rnn | GRU              | 246 K 
2 | backbone.nn  | Linear           | 25.7 K
3 | fc           | Linear           | 774   
4 | loss_fn      | CrossEntropyLoss | 0     
--------------------------------------------------
272 K     Trainable params
0         Non-trainable params
272 K     Total params
1.090     Total estimated model params size (MB)
Epoch 0/299                          0/86 0:00:00 • -:--:-- 0.00it/s v_num: cd41 val_loss:    
                                                                     1.753                    
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 579, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 986, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1032, in _run_stage
    self.fit_loop.run()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 138, in run
    self.advance(data_fetcher)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 242, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 191, in run
    self._optimizer_step(batch_idx, closure)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 269, in _optimizer_step
    call._call_lightning_module_hook(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 157, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/module.py", line 1303, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/optimizer.py", line 152, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 33, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py", line 123, in step
    loss = closure()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 108, in _wrap_closure
    closure_result = closure()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 144, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 129, in closure
    step_output = self._step_fn()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 391, in training_step
    return self.lightning_module.training_step(*args, **kwargs)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 69, in training_step
    return self.single_step(batch, batch_idx, "train")
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 57, in single_step
    self.log(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/module.py", line 516, in log
    results.log(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 406, in log
    self.register_key(key, meta, value)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 423, in register_key
    metric = _ResultMetric(meta, isinstance(value, Tensor)).to(value.device)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 311, in to
    self.__dict__.update(apply_to_collection(d, (Tensor, Metric), move_data_to_device, *args, **kwargs))
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 72, in apply_to_collection
    return _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 104, in _apply_to_collection_slow
    v = _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 104, in _apply_to_collection_slow
    v = _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 96, in _apply_to_collection_slow
    return function(data, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/apply_func.py", line 102, in move_data_to_device
    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 64, in apply_to_collection
    return function(data, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/apply_func.py", line 96, in batch_to
    data_output = data.to(device, **kwargs)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/har_classification/gru_encoder.py", line 168, in <module>
    auto_main(options)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/cli.py", line 52, in auto_main
    return pipeline.run()
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/mlflow_train.py", line 180, in run
    trainer.fit(model, datamodule)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 543, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 68, in _call_and_handle_interrupt
    trainer._teardown()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1009, in _teardown
    self.strategy.teardown()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 537, in teardown
    self.lightning_module.cpu()
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/device_dtype_mixin.py", line 82, in cpu
    return super().cpu()
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 954, in cpu
    return self._apply(lambda t: t.cpu())
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py", line 197, in _apply
    ret = super()._apply(fn)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 820, in _apply
    param_applied = fn(param)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 954, in <lambda>
    return self._apply(lambda t: t.cpu())
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [22,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [29,0,0] Assertion `t >= 0 && t < n_classes` failed.
Exception in thread Thread-2 (_pin_memory_loop):
Traceback (most recent call last):
  File "/usr/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py", line 54, in _pin_memory_loop
    do_one_step()
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py", line 31, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/usr/lib/python3.10/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/reductions.py", line 316, in rebuild_storage_fd
    fd = df.detach()
  File "/usr/lib/python3.10/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/usr/lib/python3.10/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 508, in Client
    answer_challenge(c, authkey)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 757, in answer_challenge
    response = connection.recv_bytes(256)        # reject large message
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer
 ************ Training mlp2x64 on KuHar ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name           | Type             | Params
----------------------------------------------------
0 | backbone       | Sequential       | 62.7 K
1 | backbone.fc1   | Linear           | 46.2 K
2 | backbone.relu1 | ReLU             | 0     
3 | backbone.fc2   | Linear           | 16.5 K
4 | backbone.relu2 | ReLU             | 0     
5 | fc             | Sequential       | 774   
6 | fc.0           | Linear           | 774   
7 | fc.1           | Softmax          | 0     
8 | loss_fn        | CrossEntropyLoss | 0     
----------------------------------------------------
63.5 K    Trainable params
0         Non-trainable params
63.5 K    Total params
0.254     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (11) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 200.576 seconds
Epoch 89/299 ━━━━━━━━━━━━━━━━━━━━━━ 11/11 0:00:01 • 0:00:00 398.98it/s v_num: 79b8 val_loss:  
                                                                       1.602 train_loss: 1.064
 ************ Training mlp2x64 on MotionSense ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name           | Type             | Params
----------------------------------------------------
0 | backbone       | Sequential       | 62.7 K
1 | backbone.fc1   | Linear           | 46.2 K
2 | backbone.relu1 | ReLU             | 0     
3 | backbone.fc2   | Linear           | 16.5 K
4 | backbone.relu2 | ReLU             | 0     
5 | fc             | Sequential       | 774   
6 | fc.0           | Linear           | 774   
7 | fc.1           | Softmax          | 0     
8 | loss_fn        | CrossEntropyLoss | 0     
----------------------------------------------------
63.5 K    Trainable params
0         Non-trainable params
63.5 K    Total params
0.254     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (28) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 194.935 seconds
Epoch 69/299 ━━━━━━━━━━━━━━━━━━━━━━ 28/28 0:00:01 • 0:00:00 260.09it/s v_num: 6b81 val_loss:  
                                                                       1.338 train_loss: 1.084
 ************ Training mlp2x64 on RealWorld_thigh ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name           | Type             | Params
----------------------------------------------------
0 | backbone       | Sequential       | 62.7 K
1 | backbone.fc1   | Linear           | 46.2 K
2 | backbone.relu1 | ReLU             | 0     
3 | backbone.fc2   | Linear           | 16.5 K
4 | backbone.relu2 | ReLU             | 0     
5 | fc             | Sequential       | 774   
6 | fc.0           | Linear           | 774   
7 | fc.1           | Softmax          | 0     
8 | loss_fn        | CrossEntropyLoss | 0     
----------------------------------------------------
63.5 K    Trainable params
0         Non-trainable params
63.5 K    Total params
0.254     Total estimated model params size (MB)
--> Overall fit time: 79.930 seconds
Epoch 31/299 ━━━━━━━━━━━━━━━━━━━━━━ 66/66 0:00:01 • 0:00:00 373.52it/s v_num: 0932 val_loss:  
                                                                       1.459 train_loss: 1.139
 ************ Training mlp2x64 on RealWorld_waist ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name           | Type             | Params
----------------------------------------------------
0 | backbone       | Sequential       | 62.7 K
1 | backbone.fc1   | Linear           | 46.2 K
2 | backbone.relu1 | ReLU             | 0     
3 | backbone.fc2   | Linear           | 16.5 K
4 | backbone.relu2 | ReLU             | 0     
5 | fc             | Sequential       | 774   
6 | fc.0           | Linear           | 774   
7 | fc.1           | Softmax          | 0     
8 | loss_fn        | CrossEntropyLoss | 0     
----------------------------------------------------
63.5 K    Trainable params
0         Non-trainable params
63.5 K    Total params
0.254     Total estimated model params size (MB)
--> Overall fit time: 119.252 seconds
Epoch 47/299 ━━━━━━━━━━━━━━━━━━━━━━ 81/81 0:00:01 • 0:00:00 345.83it/s v_num: cd75 val_loss:  
                                                                       1.319 train_loss: 1.103
 ************ Training mlp2x64 on UCI ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name           | Type             | Params
----------------------------------------------------
0 | backbone       | Sequential       | 62.7 K
1 | backbone.fc1   | Linear           | 46.2 K
2 | backbone.relu1 | ReLU             | 0     
3 | backbone.fc2   | Linear           | 16.5 K
4 | backbone.relu2 | ReLU             | 0     
5 | fc             | Sequential       | 774   
6 | fc.0           | Linear           | 774   
7 | fc.1           | Softmax          | 0     
8 | loss_fn        | CrossEntropyLoss | 0     
----------------------------------------------------
63.5 K    Trainable params
0         Non-trainable params
63.5 K    Total params
0.254     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (19) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 242.960 seconds
Epoch 106/299 ━━━━━━━━━━━━━━━━━━━━━━ 19/19 0:00:01 • 0:00:00 417.97it/s v_num: 5dc2 val_loss: 
                                                                        1.230 train_loss:     
                                                                        1.053                 
 ************ Training mlp2x64 on WISDM ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name           | Type             | Params
----------------------------------------------------
0 | backbone       | Sequential       | 62.7 K
1 | backbone.fc1   | Linear           | 46.2 K
2 | backbone.relu1 | ReLU             | 0     
3 | backbone.fc2   | Linear           | 16.5 K
4 | backbone.relu2 | ReLU             | 0     
5 | fc             | Sequential       | 774   
6 | fc.0           | Linear           | 774   
7 | fc.1           | Softmax          | 0     
8 | loss_fn        | CrossEntropyLoss | 0     
----------------------------------------------------
63.5 K    Trainable params
0         Non-trainable params
63.5 K    Total params
0.254     Total estimated model params size (MB)
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [11,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [15,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [16,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/pytorch/pytorch/aten/src/ATen/native/cuda/Loss.cu:241: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [26,0,0] Assertion `t >= 0 && t < n_classes` failed.
Epoch 0/299                          0/86 0:00:00 • -:--:-- 0.00it/s v_num: 8601 val_loss:    
                                                                     1.790                    
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 579, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 986, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1032, in _run_stage
    self.fit_loop.run()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 138, in run
    self.advance(data_fetcher)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 242, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 191, in run
    self._optimizer_step(batch_idx, closure)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 269, in _optimizer_step
    call._call_lightning_module_hook(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 157, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/module.py", line 1303, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/optimizer.py", line 152, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py", line 33, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py", line 123, in step
    loss = closure()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 108, in _wrap_closure
    closure_result = closure()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 144, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 129, in closure
    step_output = self._step_fn()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 391, in training_step
    return self.lightning_module.training_step(*args, **kwargs)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 69, in training_step
    return self.single_step(batch, batch_idx, "train")
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/models/nets/simple.py", line 57, in single_step
    self.log(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/module.py", line 516, in log
    results.log(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 406, in log
    self.register_key(key, meta, value)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 423, in register_key
    metric = _ResultMetric(meta, isinstance(value, Tensor)).to(value.device)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py", line 311, in to
    self.__dict__.update(apply_to_collection(d, (Tensor, Metric), move_data_to_device, *args, **kwargs))
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 72, in apply_to_collection
    return _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 104, in _apply_to_collection_slow
    v = _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 104, in _apply_to_collection_slow
    v = _apply_to_collection_slow(
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 96, in _apply_to_collection_slow
    return function(data, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/apply_func.py", line 102, in move_data_to_device
    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 64, in apply_to_collection
    return function(data, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/apply_func.py", line 96, in batch_to
    data_output = data.to(device, **kwargs)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/har_classification/mlp.py", line 132, in <module>
    auto_main(options)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/cli.py", line 52, in auto_main
    return pipeline.run()
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/mlflow_train.py", line 180, in run
    trainer.fit(model, datamodule)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 543, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 68, in _call_and_handle_interrupt
    trainer._teardown()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1009, in _teardown
    self.strategy.teardown()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py", line 537, in teardown
    self.lightning_module.cpu()
  File "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/device_dtype_mixin.py", line 82, in cpu
    return super().cpu()
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 954, in cpu
    return self._apply(lambda t: t.cpu())
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 820, in _apply
    param_applied = fn(param)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 954, in <lambda>
    return self._apply(lambda t: t.cpu())
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception in thread Thread-2 (_pin_memory_loop):
Traceback (most recent call last):
  File "/usr/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py", line 54, in _pin_memory_loop
    do_one_step()
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py", line 31, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/usr/lib/python3.10/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/reductions.py", line 316, in rebuild_storage_fd
    fd = df.detach()
  File "/usr/lib/python3.10/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/usr/lib/python3.10/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 508, in Client
    answer_challenge(c, authkey)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 752, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer
-----------------------------------------------------------------------
Running concatenated view
Entering to /workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/har_classification
 ************ Training cpc on KuHar ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name              | Type             | Params
-------------------------------------------------------
0 | encoder           | GRUEncoder       | 90.5 K
1 | encoder.rnn       | GRU              | 64.8 K
2 | encoder.nn        | Linear           | 25.7 K
3 | density_estimator | Linear           | 16.5 K
4 | auto_regressor    | GRU              | 99.1 K
5 | loss_func         | CrossEntropyLoss | 0     
-------------------------------------------------------
206 K     Trainable params
0         Non-trainable params
206 K     Total params
0.824     Total estimated model params size (MB)
--> Overall fit time: 157.802 seconds
Epoch 51/299 ━━━━━━━━━━━━━━━━━━━━━━ 57/57 0:00:01 • 0:00:00 134.92it/s v_num: 9dd3 val_loss:  
                                                                       1.952 train_loss: 1.766
 ************ Training cpc on MotionSense ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name              | Type             | Params
-------------------------------------------------------
0 | encoder           | GRUEncoder       | 90.5 K
1 | encoder.rnn       | GRU              | 64.8 K
2 | encoder.nn        | Linear           | 25.7 K
3 | density_estimator | Linear           | 16.5 K
4 | auto_regressor    | GRU              | 99.1 K
5 | loss_func         | CrossEntropyLoss | 0     
-------------------------------------------------------
206 K     Trainable params
0         Non-trainable params
206 K     Total params
0.824     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (17) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 173.029 seconds
Epoch 64/299 ━━━━━━━━━━━━━━━━━━━━━━ 17/17 0:00:01 • 0:00:00 157.83it/s v_num: 371e val_loss:  
                                                                       1.356 train_loss: 1.763
 ************ Training cpc on README.md ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name              | Type             | Params
-------------------------------------------------------
0 | encoder           | GRUEncoder       | 90.5 K
1 | encoder.rnn       | GRU              | 64.8 K
2 | encoder.nn        | Linear           | 25.7 K
3 | density_estimator | Linear           | 16.5 K
4 | auto_regressor    | GRU              | 99.1 K
5 | loss_func         | CrossEntropyLoss | 0     
-------------------------------------------------------
206 K     Trainable params
0         Non-trainable params
206 K     Total params
0.824     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/utilities/data.py:104: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.

Traceback (most recent call last):
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/har_classification/cpc.py", line 142, in <module>
    auto_main(options)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/cli.py", line 52, in auto_main
    return pipeline.run()
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/mlflow_train.py", line 180, in run
    trainer.fit(model, datamodule)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 543, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 579, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 986, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1030, in _run_stage
    self._run_sanity_check()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1061, in _run_sanity_check
    call._call_callback_hooks(self, "on_sanity_check_end")
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 208, in _call_callback_hooks
    fn(trainer, trainer.lightning_module, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/callbacks/progress/rich_progress.py", line 389, in on_sanity_check_end
    assert self.val_sanity_progress_bar_id is not None
AssertionError
 ************ Training cpc on UCI ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name              | Type             | Params
-------------------------------------------------------
0 | encoder           | GRUEncoder       | 90.5 K
1 | encoder.rnn       | GRU              | 64.8 K
2 | encoder.nn        | Linear           | 25.7 K
3 | density_estimator | Linear           | 16.5 K
4 | auto_regressor    | GRU              | 99.1 K
5 | loss_func         | CrossEntropyLoss | 0     
-------------------------------------------------------
206 K     Trainable params
0         Non-trainable params
206 K     Total params
0.824     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (21) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 187.955 seconds
Epoch 67/299 ━━━━━━━━━━━━━━━━━━━━━━ 21/21 0:00:01 • 0:00:00 126.18it/s v_num: c38b val_loss:  
                                                                       1.043 train_loss: 1.648
 ************ Training cpc on WISDM ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name              | Type             | Params
-------------------------------------------------------
0 | encoder           | GRUEncoder       | 90.5 K
1 | encoder.rnn       | GRU              | 64.8 K
2 | encoder.nn        | Linear           | 25.7 K
3 | density_estimator | Linear           | 16.5 K
4 | auto_regressor    | GRU              | 99.1 K
5 | loss_func         | CrossEntropyLoss | 0     
-------------------------------------------------------
206 K     Trainable params
0         Non-trainable params
206 K     Total params
0.824     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (36) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 112.158 seconds
Epoch 38/299 ━━━━━━━━━━━━━━━━━━━━━━ 36/36 0:00:01 • 0:00:00 117.88it/s v_num: 08bd val_loss:  
                                                                       1.754 train_loss: 1.917
 ************ Training cpc on generator_view_ssl.ipynb ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name              | Type             | Params
-------------------------------------------------------
0 | encoder           | GRUEncoder       | 90.5 K
1 | encoder.rnn       | GRU              | 64.8 K
2 | encoder.nn        | Linear           | 25.7 K
3 | density_estimator | Linear           | 16.5 K
4 | auto_regressor    | GRU              | 99.1 K
5 | loss_func         | CrossEntropyLoss | 0     
-------------------------------------------------------
206 K     Trainable params
0         Non-trainable params
206 K     Total params
0.824     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/utilities/data.py:104: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.

Traceback (most recent call last):
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/har_classification/cpc.py", line 142, in <module>
    auto_main(options)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/cli.py", line 52, in auto_main
    return pipeline.run()
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/mlflow_train.py", line 180, in run
    trainer.fit(model, datamodule)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 543, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 579, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 986, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1030, in _run_stage
    self._run_sanity_check()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1061, in _run_sanity_check
    call._call_callback_hooks(self, "on_sanity_check_end")
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 208, in _call_callback_hooks
    fn(trainer, trainer.lightning_module, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/callbacks/progress/rich_progress.py", line 389, in on_sanity_check_end
    assert self.val_sanity_progress_bar_id is not None
AssertionError
 ************ Training cpc on plot_data ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name              | Type             | Params
-------------------------------------------------------
0 | encoder           | GRUEncoder       | 90.5 K
1 | encoder.rnn       | GRU              | 64.8 K
2 | encoder.nn        | Linear           | 25.7 K
3 | density_estimator | Linear           | 16.5 K
4 | auto_regressor    | GRU              | 99.1 K
5 | loss_func         | CrossEntropyLoss | 0     
-------------------------------------------------------
206 K     Trainable params
0         Non-trainable params
206 K     Total params
0.824     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/utilities/data.py:104: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.

Traceback (most recent call last):
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/har_classification/cpc.py", line 142, in <module>
    auto_main(options)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/cli.py", line 52, in auto_main
    return pipeline.run()
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/mlflow_train.py", line 180, in run
    trainer.fit(model, datamodule)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 543, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 579, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 986, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1030, in _run_stage
    self._run_sanity_check()
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 1061, in _run_sanity_check
    call._call_callback_hooks(self, "on_sanity_check_end")
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 208, in _call_callback_hooks
    fn(trainer, trainer.lightning_module, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/callbacks/progress/rich_progress.py", line 389, in on_sanity_check_end
    assert self.val_sanity_progress_bar_id is not None
AssertionError
 ************ Training tnc on KuHar ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name                  | Type              | Params
------------------------------------------------------------
0 | discriminator         | TNCDiscriminator  | 881   
1 | discriminator.model   | Sequential        | 881   
2 | discriminator.model.0 | Linear            | 840   
3 | discriminator.model.1 | ReLU              | 0     
4 | discriminator.model.2 | Dropout           | 0     
5 | discriminator.model.3 | Linear            | 41    
6 | encoder               | GRUEncoder        | 66.8 K
7 | encoder.rnn           | GRU               | 64.8 K
8 | encoder.nn            | Linear            | 2.0 K 
9 | loss_func             | BCEWithLogitsLoss | 0     
------------------------------------------------------------
67.7 K    Trainable params
0         Non-trainable params
67.7 K    Total params
0.271     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 2336.193 seconds
Epoch 123/299 ━━━━━━━━━━━━━━━━━━━━━━━━ 3/3 0:00:13 • 0:00:00 4.35it/s v_num: 4761 val_loss:   
                                                                      0.614 train_loss: 0.612 
 ************ Training tnc on MotionSense ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name                  | Type              | Params
------------------------------------------------------------
0 | discriminator         | TNCDiscriminator  | 881   
1 | discriminator.model   | Sequential        | 881   
2 | discriminator.model.0 | Linear            | 840   
3 | discriminator.model.1 | ReLU              | 0     
4 | discriminator.model.2 | Dropout           | 0     
5 | discriminator.model.3 | Linear            | 41    
6 | encoder               | GRUEncoder        | 66.8 K
7 | encoder.rnn           | GRU               | 64.8 K
8 | encoder.nn            | Linear            | 2.0 K 
9 | loss_func             | BCEWithLogitsLoss | 0     
------------------------------------------------------------
67.7 K    Trainable params
0         Non-trainable params
67.7 K    Total params
0.271     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 1207.292 seconds
Epoch 99/299 ━━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:10 • 0:00:00 0.00it/s v_num: e373 val_loss: 
                                                                  0.505 train_loss:     
                                                                  0.490                 
 ************ Training tnc on README.md ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
Traceback (most recent call last):
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/har_classification/tnc.py", line 143, in <module>
    auto_main(options)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/cli.py", line 52, in auto_main
    return pipeline.run()
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/mlflow_train.py", line 180, in run
    trainer.fit(model, datamodule)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 543, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 579, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 948, in _run
    call._call_setup_hook(self)  # allow user to set up LightningModule in accelerator environment
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 92, in _call_setup_hook
    _call_lightning_datamodule_hook(trainer, "setup", stage=fn)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 179, in _call_lightning_datamodule_hook
    return fn(*args, **kwargs)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/data/data_modules/har.py", line 248, in setup
    self.datasets["train"] = self._load_dataset("train")
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/data/data_modules/har.py", line 442, in _load_dataset
    har_dataset = super()._load_dataset(split_name)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/data/data_modules/har.py", line 220, in _load_dataset
    return SeriesFolderCSVDataset(
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/data/datasets/series_dataset.py", line 312, in __init__
    self._longest_sample_size = self._get_longest_sample_size()
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/data/datasets/series_dataset.py", line 345, in _get_longest_sample_size
    longest_sample_size = max(
ValueError: max() arg is an empty sequence
 ************ Training tnc on UCI ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name                  | Type              | Params
------------------------------------------------------------
0 | discriminator         | TNCDiscriminator  | 881   
1 | discriminator.model   | Sequential        | 881   
2 | discriminator.model.0 | Linear            | 840   
3 | discriminator.model.1 | ReLU              | 0     
4 | discriminator.model.2 | Dropout           | 0     
5 | discriminator.model.3 | Linear            | 41    
6 | encoder               | GRUEncoder        | 66.8 K
7 | encoder.rnn           | GRU               | 64.8 K
8 | encoder.nn            | Linear            | 2.0 K 
9 | loss_func             | BCEWithLogitsLoss | 0     
------------------------------------------------------------
67.7 K    Trainable params
0         Non-trainable params
67.7 K    Total params
0.271     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 2907.834 seconds
Epoch 208/299 ━━━━━━━━━━━━━━━━━━━━ 1/1 0:00:10 • 0:00:00 0.00it/s v_num: bb9a val_loss: 
                                                                  0.496 train_loss:     
                                                                  0.525                 
 ************ Training tnc on WISDM ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]

  | Name                  | Type              | Params
------------------------------------------------------------
0 | discriminator         | TNCDiscriminator  | 881   
1 | discriminator.model   | Sequential        | 881   
2 | discriminator.model.0 | Linear            | 840   
3 | discriminator.model.1 | ReLU              | 0     
4 | discriminator.model.2 | Dropout           | 0     
5 | discriminator.model.3 | Linear            | 41    
6 | encoder               | GRUEncoder        | 66.8 K
7 | encoder.rnn           | GRU               | 64.8 K
8 | encoder.nn            | Linear            | 2.0 K 
9 | loss_func             | BCEWithLogitsLoss | 0     
------------------------------------------------------------
67.7 K    Trainable params
0         Non-trainable params
67.7 K    Total params
0.271     Total estimated model params size (MB)
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
--> Overall fit time: 2779.529 seconds
Epoch 174/299 ━━━━━━━━━━━━━━━━━━━━ 2/2 0:00:12 • 0:00:00 47.21it/s v_num: ffde val_loss:
                                                                   0.371 train_loss:    
                                                                   0.489                
 ************ Training tnc on generator_view_ssl.ipynb ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
Traceback (most recent call last):
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/har_classification/tnc.py", line 143, in <module>
    auto_main(options)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/cli.py", line 52, in auto_main
    return pipeline.run()
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/mlflow_train.py", line 180, in run
    trainer.fit(model, datamodule)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 543, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 579, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 948, in _run
    call._call_setup_hook(self)  # allow user to set up LightningModule in accelerator environment
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 92, in _call_setup_hook
    _call_lightning_datamodule_hook(trainer, "setup", stage=fn)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 179, in _call_lightning_datamodule_hook
    return fn(*args, **kwargs)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/data/data_modules/har.py", line 248, in setup
    self.datasets["train"] = self._load_dataset("train")
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/data/data_modules/har.py", line 442, in _load_dataset
    har_dataset = super()._load_dataset(split_name)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/data/data_modules/har.py", line 220, in _load_dataset
    return SeriesFolderCSVDataset(
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/data/datasets/series_dataset.py", line 312, in __init__
    self._longest_sample_size = self._get_longest_sample_size()
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/data/datasets/series_dataset.py", line 345, in _get_longest_sample_size
    longest_sample_size = max(
ValueError: max() arg is an empty sequence
 ************ Training tnc on plot_data ************
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
Traceback (most recent call last):
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/har_classification/tnc.py", line 143, in <module>
    auto_main(options)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/cli.py", line 52, in auto_main
    return pipeline.run()
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/pipelines/mlflow_train.py", line 180, in run
    trainer.fit(model, datamodule)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 543, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 579, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py", line 948, in _run
    call._call_setup_hook(self)  # allow user to set up LightningModule in accelerator environment
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 92, in _call_setup_hook
    _call_lightning_datamodule_hook(trainer, "setup", stage=fn)
  File "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py", line 179, in _call_lightning_datamodule_hook
    return fn(*args, **kwargs)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/data/data_modules/har.py", line 248, in setup
    self.datasets["train"] = self._load_dataset("train")
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/data/data_modules/har.py", line 442, in _load_dataset
    har_dataset = super()._load_dataset(split_name)
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/data/data_modules/har.py", line 220, in _load_dataset
    return SeriesFolderCSVDataset(
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/data/datasets/series_dataset.py", line 312, in __init__
    self._longest_sample_size = self._get_longest_sample_size()
  File "/workspaces/hiaac-m4/ssl_tools/ssl_tools/data/datasets/series_dataset.py", line 345, in _get_longest_sample_size
    longest_sample_size = max(
ValueError: max() arg is an empty sequence
